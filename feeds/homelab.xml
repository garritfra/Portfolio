<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>
            garrit.xyz
        </title>
        <link>
            https://garrit.xyz
        </link>
        <description>
            Garrit Franke
        </description>
        <language>
            en
        </language>
        <lastBuildDate>
            Thu, 27 Apr 2023 00:00:00 +0000
        </lastBuildDate>
        <item>
            <title>
                Migrating Homeassistant from SD to SSD
            </title>
            <guid>
                https://garrit.xyz/posts/2023-04-27-migrating-homeassistant-from-sd-to-ssd
            </guid>
            <link>
                https://garrit.xyz/posts/2023-04-27-migrating-homeassistant-from-sd-to-ssd?utm_source=rss
            </link>
            <pubDate>
                Thu, 27 Apr 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I finally got frustrated with the performance of my Raspberry Pi 4 running Homeassistant on a SD card, so I went ahead and got an SSD.</p>

<p>The migration was <strong>very</strong> easy:</p>

<ol><li>Create and download a full backup through the UI</li><li>Flash Homeassistant onto the SSD</li><li>Remove the SD card and plug the SSD into a USB 3.0 port of the Pi</li><li>Boot</li><li>Go through the onboarding procedure</li><li>Restore Backup</li><li>Profit</li></ol>

<p>It worked like a charm! The speed has improved A LOT, and everything was set up as it should be. </p>

<p>...Until we turned on the lights in the livingroom. My ZigBee-dongle, plugged into another USB port, wasn&#39;t able to communicate with the devices on the network.</p>

<p>After some digging around, I came across several threads stating that an SSD over USB 3.0 apparently creates a lot of interference to surrounding hardware, including my ZigBee dongle. The fix was simple: either get an extension port for the dongle, or plug the SSD into a USB 2.0 port of the Pi. Since I didn&#39;t have an extension cord to get the dongle far away enough from the SSD, I went with the latter option for now. And that fixed it! The performance was much worse, but still better than the SD I used before. My next step will be to grab an extension cord from my parents. I&#39;m sure they won&#39;t mind.</p>

<p>I hope this helps!</p>

<hr/>

<p>This is post 066 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Self-hosted software I'm thankful for
            </title>
            <guid>
                https://garrit.xyz/posts/2022-09-26-self-hosted-software-im-thankful-for
            </guid>
            <link>
                https://garrit.xyz/posts/2022-09-26-self-hosted-software-im-thankful-for?utm_source=rss
            </link>
            <pubDate>
                Mon, 26 Sep 2022 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>Self-hosting software is not just rainbows and sunshine. I used to self-host a lot of my tools, but after some time the burden of maintaining those tools made me switch to hosted alternatives.</p>

<p>However, there are a few projects that I stuck with over the years, and which I think deserve a deep appreciation.</p>

<h2>Miniflux</h2>

<p><a href="https://miniflux.app/">Miniflux</a> is a very minimal, self-hostable RSS reader. It&#39;s been rock-solid since they day I started using it. The data for the application entirely lives In a Postgres database, which makes migrating the application to new infrastructure setups an absolute breeze. I&#39;ve been meaning to support the author for quite some time now, but the cost of maintaining an instance yourself is basically zero, so I&#39;ve yet to find the time to switch to their paid hosted instance.</p>

<h2>Plausible Analytics</h2>

<p><a href="https://plausible.io/">Plausible</a> is another tool that just keeps on running. I haven&#39;t had any issues with it whatsoever, and I can&#39;t remember the last time I had to do a manual intervention. Just like Miniflux, there&#39;s a paid instance, which supports the author, and just like Miniflux, the software is so good that I haven&#39;t had a reason to switch to it yet. Oh, the irony.</p>

<h2>BirdsiteLIVE</h2>

<p>While my instance of <a href="https://birdsite.slashdev.space/">BirdsiteLIVE</a> is currently in a bad shape, this is not at all the fault of the software. There are limitations to the amount of Twitter API requests you can make, and I did a poor job managing the users on that instance. It&#39;s currently very overloaded and just very few tweets make it through. I will have to set aside some time to fix this, but the software itself has been rock solid since the day I started using it.</p>

<h2>Synapse (Matrix)</h2>

<p>I was hesitant to mention <a href="https://matrix.org/">Matrix</a> on this list. I had my ups and downs with Synapse (their Python implementation of a Matrix server), but the fact that my instance is still running after multiple infrastructure transitions and even a migration from SQLite to Postgres says something about the quality of the software. I have a feeling that Synapse is fairly resource-hungry, but if you feed it with enough RAM and disk, it will keep running indefinitely.</p>

<h2>Homeassistant</h2>

<p>You can throw <a href="https://www.home-assistant.io/">Homeassistant</a> on a Raspberry Pi and everything works out of the gate. I even migrated my instance from a RPi 3 to a RPi 4 via their backup and restore functionality. It&#39;s absolutely flawless.</p>

<h2>Dead projects</h2>

<p>I think it&#39;s fair to also mention the software that I no longer self-host.</p>

<h3>E-Mail</h3>

<p>Just don&#39;t roll your own email.</p>

<h3>Mastodon</h3>

<p>Too power hungry for my taste. No easy way to host inside docker, which made it a pain to keep running. I&#39;m very happy with <a href="https://fosstodon.org/">Fosstodon</a>, and don&#39;t see a reason to switch to a self-hosted instance any time soon.</p>

<h3>FreshRSS</h3>

<p>I tried replacing Miniflux once, but failed. Nothing beats Miniflux.</p>

<h3>Prometheus + Grafana</h3>

<p>Monitoring <strong><em>inside</em></strong> your infra works until the infra goes down, at which point you&#39;re essentially driving blindfolded. I switched to Grafana Cloud, which includes a very generous free tier.</p>

<p>This is post 038 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Postgres Docker Container Migration Cheat Sheet
            </title>
            <guid>
                https://garrit.xyz/posts/2022-05-31-database-server-migration-cheat-sheet
            </guid>
            <link>
                https://garrit.xyz/posts/2022-05-31-database-server-migration-cheat-sheet?utm_source=rss
            </link>
            <pubDate>
                Tue, 31 May 2022 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I just finished migrating a postgres database to a new host. To remember how to
do it next time, I&#39;m writing down the commands I used here.</p>

<p>I usually just shut down the database and then copy the local directory where
the volume was mounted onto the new host. This time though, I seemed to be
getting some I/O errors, so I had to do it the &quot;right&quot; way.</p>

<p>To be fair, this note is based on
<a href="https://www.netguru.com/blog/how-to-dump-and-restore-postgresql-database">this</a>
guide. I modified it to fit my workflow with docker.</p>

<h2>Creating a dump</h2>

<p>Log into the old host:</p>

<p><code>
ssh &lt;user&gt;@host
</code></p>

<p>Connect to the postgres-container:</p>

<p><code>
docker exec -ti myservice_db_1 /bin/bash
</code></p>

<p>Create a dump. You can name your dump as you wish - I&#39;m using dates to
distinguish multiple dumps:</p>

<p><code>
pg_dump -U db_user db_name &gt; db_name_20220531.sql
</code></p>

<p>Copy the dump to the host machine:</p>

<p><code>
docker cp myservice_db_1:/db_name_20220531.sql ~/
</code></p>

<h2>Moving the dump to the new host</h2>

<p>The easiest way to get the dump off of the old server and onto the new one is to
use your local machine as a middleman.</p>

<p>First, download the dump to your machine:</p>

<p><code>
scp &lt;user&gt;@&lt;host&gt;:~/db_name_20220531.sql .
</code></p>

<p>Then, do the same thing but reversed, with the new host:</p>

<p><code>
scp ./db_name_20220531.sql &lt;user&gt;@&lt;host&gt;:~/
</code></p>

<h2>Restoring the dump</h2>

<p>First, connect to the new host:</p>

<p><code>
ssh &lt;user&gt;@&lt;host&gt;
</code></p>

<p>Assuming the docker service is already running on the new host, attach to the
db-container, just like above:</p>

<p><code>
docker exec -ti myservice_db_1 /bin/bash
</code></p>

<p>This time, we have to do some fiddling on the database, so attach a session to
postgres using their cli:</p>

<p><code>
psql -U my_user
</code></p>

<p>Before &quot;resetting&quot; the existing DB to apply the dump, we have to connect to
another database. The <code>postgres</code> DB is always there, so you can use that.</p>

<p><code>
\c postgres
</code></p>

<p>Now, we drop the existing DB and re-add it:</p>

<p><code>sql
drop database database_name;
create database database_name with owner your_user_name;
</code></p>

<p>And now, the moment you&#39;ve been waiting for! Leave the psql-session and apply
the dump:</p>

<p><code>
psql -U db_user db_name &lt; db_name_20220531.sql
</code></p>

<p>That&#39;s all! You now have the exact copy of production database available on your
machine.</p>

<p>This is post 032 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Fixing Traefik Proxy Issues
            </title>
            <guid>
                https://garrit.xyz/posts/2022-03-18-fix-traefik-proxy-issues
            </guid>
            <link>
                https://garrit.xyz/posts/2022-03-18-fix-traefik-proxy-issues?utm_source=rss
            </link>
            <pubDate>
                Fri, 18 Mar 2022 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>After changing my proxy from NGINX to Traefik, I noticed that some of my
services started misbehaving.</p>

<p>In particular, my instance of
<a href="https://github.com/NicolasConstant/BirdsiteLive">BirdsiteLive</a>
(<a href="https://birdsite.slashdev.space">birdsite.slashdev.space</a>) had issues
forwarding tweets to the
<a href="https://garrit.xyz/posts/2021-01-18-reasons-the-fediverse-is-better">Fediverse</a>.</p>

<p>The only difference between my old NGINX and my Traefik config were the headers.
I didn&#39;t think that that&#39;s what&#39;s causing the issue, but after digging around a
bit I figured out what&#39;s wrong. I still can&#39;t wrap my head around it entirely,
but it has something to do with forwarding external <code>https</code> requests to internal
<code>http</code> services, since the <code>x-forwarded-</code> headers where missing in the forwarded
requests.</p>

<p>In the world of NGINX, we can instruct the proxy to forward <em>all</em> headers using
this directive:</p>

<p><code>conf
proxy_pass_request_headers      on;
</code></p>

<p>which takes care of the issue. In Traefik, it&#39;s a bit more convoluted. Traefik
can use a combination of &quot;Entrypoints&quot; and middleware to route traffic around.
In my setup, I use a <code>webSecure</code> entrypoint listening for SSL/TLS traffic, and a
<code>web</code> entrypoint that just redirects to <code>webSecure</code>:</p>

<p><code></code>`yaml
entryPoints:
  web:
    address: :80
    http:
      redirections:
        entryPoint:
          to: &quot;websecure&quot;
          scheme: &quot;https&quot;</p>

<p>  websecure:
    address: :443
<code></code>`</p>

<p>Apparently, some services send requests to the <code>web</code> entrypoint, and the
<code>x-forwarded-for</code> headers are dropped. To prevent this, you can set the
<code>proxyProtocol</code> and <code>forwardedHeaders</code> in the <code>web</code> entrypoint to <code>insecure</code>,
like so:</p>

<p><code></code>`yaml
entryPoints:
  web:
    address: :80
    proxyProtocol:
      insecure: true
    forwardedHeaders:
      insecure: true
    # ...</p>

<h1>...</h1>

<p><code></code>`</p>

<p>I&#39;m sure there&#39;s a reason why this is marked as <code>insecure</code>, but it behaves just
like the NGINX counterpart, so I didn&#39;t bother digging deeper into the matter.
Maybe one day I&#39;ll come back to properly fix this.</p>

<p>If you want to read more, check out
<a href="https://medium.com/@_jonas/traefik-kubernetes-ingress-and-x-forwarded-headers-82194d319b0e">this</a>
article on Medium. It explains the issue in more detail.</p>

<hr/>

<p>This is post 025 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                BTRFS on Alpine Linux
            </title>
            <guid>
                https://garrit.xyz/posts/2021-12-31-btrfs-on-alpine
            </guid>
            <link>
                https://garrit.xyz/posts/2021-12-31-btrfs-on-alpine?utm_source=rss
            </link>
            <pubDate>
                Fri, 31 Dec 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I&#39;m currently in the midst of migrating some of my infrastructure from the cloud
to &quot;on prem&quot;, aka a local server, aka my old PC. I wanted to try alpine linux as
the host OS to see how it behaves as a lightweight server distro.</p>

<p>So far it stands up quite nicely, it has everything you&#39;d expect from a
linux-based operating system. The only problem I encountered was getting BTRFS
to work out of the box. Here are some things you should know when using BTRFS on
Alpine linux.</p>

<h3>Installing BTRFS</h3>

<p>Installing BTRFS is relatively straight forward. Simply install the package and
tell Alpine to load the module on startup:</p>

<p><code>
apk add btrfs-progs
echo btrfs &gt;&gt; /etc/modules
</code></p>

<p>To load the module right away, you can use the following command:</p>

<p><code>
modprobe btrfs
</code></p>

<h3>Mounting a volume</h3>

<p>If you try mounting a btrfs volume via your fstab, you will get an error. This
is because BTRFS does not know about the drives yet when the filesystems are
mounted. To work around this, you can create an OpenRC service that runs a
<code>btrfs scan</code> to detect the drives. To do so, create a service under
<code>/etc/init.d/btrfs-scan</code> with the following content:</p>

<p><code></code>`sh</p>

<h1>!/sbin/openrc-run</h1>

<p>name=&quot;btrfs-scan&quot;</p>

<p>depend() {
  before localmount
}</p>

<p>start() {
  /sbin/btrfs device scan
}
<code></code>`</p>

<p>Make the service executable and register it:</p>

<p><code>
chmod +x /etc/init.d/btrfs-scan
rc-update add btrfs-scan boot
</code></p>

<p>Now, you should be able to add the volume to your <code>/etc/fstab</code>:</p>

<p><code>
UUID=abcdef-0055-4958-990f-1413ed1186ec  /var/data  btrfs   defaults,nofail,subvol=@  0  0
</code></p>

<p>After a reboot, you should be able to see the drive mounted at <code>/var/data</code>.</p>

<h3>Resources</h3>

<ul><li><a href="https://nparsons.uk/blog/using-btrfs-on-alpine-linux">Nathan Parsons - &quot;Using BTRFS on Alpine Linux&quot;</a></li><li><a href="https://gitlab-test.alpinelinux.org/alpine/aports/-/issues/9539">A bug report about this problem</a></li></ul>

<p>This is post 023 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Finally fixing that annoying Cron gotcha
            </title>
            <guid>
                https://garrit.xyz/posts/2021-09-13-fixing-an-annoying-cron-gotcha
            </guid>
            <link>
                https://garrit.xyz/posts/2021-09-13-fixing-an-annoying-cron-gotcha?utm_source=rss
            </link>
            <pubDate>
                Mon, 13 Sep 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>A while ago I went through my server and reworked my <a href="/posts/2021-02-07-storage-setup">storage
setup</a>. As discribed in that blog post, I set
up daily backups to <a href="https://www.backblaze.com/cloud-storage">Backblaze
B2</a> using their amazing CLI
through a cron script. A day went by and I noticed that the
<a href="/posts/2021-05-15-healthchecks-io-with-docker">healthcheck</a> didn&#39;t pass.
Unfortunately I didn&#39;t have time to fix this problem immediately, so instead I
executed the command by hand every couple of days. One could argue that this in
total took way more time than the actual fix, but hey, I was lazy. In the end,
I finally dedicated some time to fix this annoying issue.</p>

<p>It turns out that a command executed by cron doesn&#39;t run through sh or bash,
but in a minimal environment without your usual environment-variables. As a
result, my <code>b2</code> command (and many other commands for that matter) won&#39;t run as
expected, if at all. A quick fix is to run your command through bash or sh
explicitly:</p>

<p><code>sh
sh -c &quot;mycommand&quot;
</code></p>

<p>Alternatively, if you want all your entries to use sh or bash, you can set the
<code>SHELL</code> variable at the very beginning of your crontab:</p>

<p><code></code>`sh
SHELL=/bin/bash</p>

<p>15 1 <em> </em> * some_command
<code></code>`</p>

<p><a href="https://askubuntu.com/a/23438">Here</a> is an answer that goes into more detail
about this. Have a great day!</p>

<p>This is post 019 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Docker healthchecks using healthchecks.io
            </title>
            <guid>
                https://garrit.xyz/posts/2021-05-15-healthchecks-io-with-docker
            </guid>
            <link>
                https://garrit.xyz/posts/2021-05-15-healthchecks-io-with-docker?utm_source=rss
            </link>
            <pubDate>
                Sat, 15 May 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I&#39;m currently in the midst of improving the monitoring of my infrastructure. I
make heavy use of docker and docker-compose for my hosting, so it&#39;s vital to add
monitoring for most of the containers.</p>

<p>I&#39;m hosting my own instance of <a href="https://healthchecks.io/">healthchecks.io</a>.
Their solution to monitoring involves <strong>you</strong> having to ping <strong>them</strong>, instead
of the other way around. This let&#39;s you add healthchecks to virtually anything
that can ping a http-endpoint.</p>

<p>docker-compose let&#39;s you define healthchecks to your config that, when
completing sucessfully, mark the container as &quot;healthy&quot;. The process of adding
such a healthcheck is simple. First, create a new check in your healthchecks.io
account and set the ping interval to 1 minute, or a value you prefer. Then, add
this snippet to your docker-compose file:</p>

<p><code>yaml
app:
  image: nextcloud
  ports:
    - 127.0.0.1:8080:80
  healthcheck:
    test:
      [
        &quot;CMD&quot;,
        &quot;curl&quot;,
        &quot;-f&quot;,
        &quot;https://app-endpoint.tld&quot;,
        &quot;&amp;&amp;&quot;,
        &quot;curl&quot;,
        &quot;-fsS&quot;,
        &quot;-m&quot;,
        &quot;10&quot;,
        &quot;--retry&quot;,
        &quot;5&quot;,
        &quot;-o&quot;,
        &quot;/dev/null&quot;,
        &quot;https://healthchecks.io/ping/&lt;UUID&gt;&quot;,
      ]
    interval: 60s
    timeout: 10s
    retries: 6
</code></p>

<p>Change the first url to the url of your app. The second URL is the endpoint of
your healthchecks.io instance. You can obtain it from the check you configured
earlier.</p>

<p>This configuration will try to ping your application and, if successful, notify
the healthcheck that the application is healthy. If the app is not reachable or
the container is down, the latter request will not be executed and your service
is marked as &quot;down&quot;.</p>

<p>In addition to the healthchecks of my docker containers, I also added basic
healthchecks to my servers cronfiles and its backup-commands.</p>

<p>Do you have any suggestions regarding this topic? Feel free to reach out to me
via Matrix or email!</p>

<p>This is post 017 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                My storage setup (Feburary 2021)
            </title>
            <guid>
                https://garrit.xyz/posts/2021-02-07-storage-setup
            </guid>
            <link>
                https://garrit.xyz/posts/2021-02-07-storage-setup?utm_source=rss
            </link>
            <pubDate>
                Sun, 07 Feb 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I used to rely on Google Drive and Photos to store my entire data. Now, that <a href="https://blog.google/products/photos/storage-changes/">Google has decided to ditch unlimited photo storage in the near future</a> and Google basically being the devil himself, I decided to step up my game and get my hands dirty on a DIY storage solution.</p>

<h2>The goal</h2>

<p>Before I got started, I thought about the expectations I have towards a system like this. It boils down to these four points (in this order): I want my solution to be <strong>resiliant</strong>, <strong>scalable</strong>, <strong>easy to maintain</strong> and <strong>easy to access</strong>. Looking back, I think I met all of these requirements fairly well. Let me walk you through how I managed to do that.</p>

<h2>Data resiliance</h2>

<p>Keeping data on a single device is obviously a really bad idea. Drives eventually fail, which means that your data will be lost. Heck, even my house could burn down, which means that any number of local copies could burn to ashes. To prevent data loss, I strictly adhere to the <a href="https://www.backblaze.com/blog/the-3-2-1-backup-strategy/">3-2-1 backup strategy</a>. A 3-2-1 strategy means having <strong>at least three total copies of your data, two of which are local but on different mediums (read: devices), and at least one copy off-site</strong>. If a drive fails, I can replace it. If my house burns down, I get two new drives and clone my offsite backup to them.</p>

<p>To get an offsite backup, I set up a spare Raspberry Pi with a single large HDD and instructed it to do daily backups of my entire data. I asked a family member if they would be willing to have a tiny computer plugged in to their router 24/7, and they kindly agreed. A Pi and a HDD are very efficient in terms of power, so there is not a lot to worry about.</p>

<h2>Scalability</h2>

<p>I currently don&#39;t have a huge amount of data. If that were to change (i.e. if I continue to shoot a lot of high-res photos and shove them into my setup), I need a way to simply attach more drives, or ones with more capacity. I looked at different file-systems that allowed to easy extendability while also being resiliant.</p>

<p>An obvious candidate was <strong>ZFS</strong>, but there are a couple of reasons I ditched this idea. First of all, it is really hard to get up and running on Raspberry Pi running Linux, since it&#39;s not natively supported by all distributions. This increases the complexity of the setup. Another reason is that I don&#39;t like the way it scales. Please correct me if I&#39;m wrong here, since I only did limited research on this. From what I know though, ZFS can only be extended by shoving a large amount of drives in the setup to achieve perfect redundancy.</p>

<p>In the end, I settled on <strong>BTRFS</strong>. For me, it scratches all the itches that ZFS has. It is baked into the linux kernel, which makes it really easy to install on most distributions, and I can scale it to any number of drives I want. If I find a spare drive somewhere with any storage capacity, I can plug it into the system and it will just work, without having to think about balancing or redundancy shenanigans.</p>

<h2>Maintainability</h2>

<p>I need my setup to be easy to maintain. If a drive fails, I want to be able to replace it within a matter of minutes, not hours. If my host (a Raspberry Pi) bursts into flames, I want to be able to swap in a new one and still access my data. If I&#39;m out and about and something goes south, I want to be able to fix it remotely. BTRFS helps a lot here. It&#39;s really the foundation for all the key points mentioned here. It gives me a simple interface to maintain the data on the drives, and tries to fix issues itself whenever possible.</p>

<p>Exposing random ports to the general public is a huge security risk. To still be able to access the Pi remotely, I set up <strong>an encrypted WireGuard tunnel</strong>. This way, I only have to expose a single port for WireGuard to talk to the device as if I&#39;m sitting next to it.</p>

<h2>Accessibility</h2>

<p>Since the data needs to be accessed frequently, I need a simple interface for it that can be used on any device. I decided to host a <strong>Nextcloud</strong> instance and mount the drive as external storage. Why external storage? Because Nextcloud does some weird thing with the data it stores. If I decide to ditch Nextcloud at some point, I have the data on the disks &quot;as is&quot;, without some sort of abstraction on top of it. This also has the benefit of allowing access from multiple sources. I don&#39;t have to use Nextcloud, but instead can mount the volume as a FTP, SMB or NFS share and do whatever I want with it. From the nextcloud perspective, this has some drawbacks like inefficient caching or file detection, but I&#39;m willing to make that tradeoff.</p>

<h2>In a nutshell</h2>

<p>This entire setup cost me about 150€ in total. Some components were scraped from old PC parts. So, what does the solution look like? Here is the gist:</p>

<ul><li>A Raspberry Pi 4 as a main host and an older Raspberry Pi 3 for offsite backup, both running Raspberry Pi OS</li><li>Two external harddrives in a RAID 1 (mirrored) configuration, running on an external USB3 hub</li><li>A single internal HDD that served no purpose in my old PC, now serving as backup storage</li><li>All drives are using BTRFS</li><li>WireGuard tunnels between main and remote host, as well as most access devices</li><li>Nextcloud on main host, accessible over TLS (if I need to access data from outside the secure tunnel-network)</li><li>SMB share accessible from within the tunnel-network</li><li>Circa 4.5 terabyte total disk size; 1.5 terabyte of usable storage</li><li>Snapper for local incremental backups on main host; BTRBK for remote incremental backups</li><li>Cron jobs for regular backups and repairs (scrub/rebalance)</li></ul>

<p>This is post 010 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Quick Tip! Setting up a lightweight Server-Client VPN with wireguard
            </title>
            <guid>
                https://garrit.xyz/posts/lightweight-vpn-with-wireguard
            </guid>
            <link>
                https://garrit.xyz/posts/lightweight-vpn-with-wireguard?utm_source=rss
            </link>
            <pubDate>
                Wed, 19 Aug 2020 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>This blog post has been taken over from my <a href="https://garrit.xyz/til">collection of &quot;Today I Learned&quot; articles</a>.</p>

<p>You can easily set up a private network of your devices. This way you can &quot;talk&quot; to your phone, raspberry pi etc. over an <strong>encrypted</strong> network, with simple IP-addresses.</p>

<p><img alt="" src="https://images.unsplash.com/photo-1505659903052-f379347d056f?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=2550&amp;q=80"/></p>

<p>Firstly, install wireguard on all of your systems. Simply install the <code>wireguard</code> package from your package manager respectively. Check out <a href="https://www.wireguard.com/install/">the official installation guide</a> if you can&#39;t find the package. If you&#39;re on debian, try <a href="https://wiki.debian.org/WireGuard?action=show&amp;redirect=Wireguard">this</a> guide. There&#39;s also an app for Android, iOS and MacOS.</p>

<p>Every participent (Client and server) needs a key-pair. To generate this, run this command first on the server, and on all clients:</p>

<p><code>bash
wg genkey | tee wg-private.key | wg pubkey &gt; wg-public.key
</code></p>

<p>It might make sense to do this as root. This way you don&#39;t have to type <code>sudo</code> with every command.</p>

<h2>Server Configuration</h2>

<p>You will need to create a configuration for the server. Save this template at <code>/etc/wireguard/wg0.conf</code>, and replace the fields where needed:</p>

<p><code></code>`conf
[Interface]
PrivateKey = &lt;Server private key from wg-private.key&gt;
Address = 10.0.0.1/24 # IP Address of the server. Using this IP Address, you can assign IPs ranging from 10.0.0.2 - 10.0.0.254 to your clients
ListenPort = 51820 # This is the standard port for wireguard</p>

<h1>The following fields will take care of routing</h1>

<p>PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE
PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE</p>

<h1>Laptop</h1>

<p>[Peer]
PublicKey = &lt;Public Key of Laptop Client&gt;
AllowedIPs = 10.0.0.2/32 # The client will be reachable at this address</p>

<h1>Android Phone</h1>

<p>[Peer]
PublicKey = &lt;Public Key of Phone Client&gt;
AllowedIPs = 10.0.0.3/32</p>

<h1>...</h1>

<p><code></code>`</p>

<p>Then run <code>wg-quick up wg0</code> to start the wireguard interface with the configuration from <code>/etc/wireguard/wg0</code>.</p>

<h2>Setting up clients</h2>

<p>Setting up clients is very similar to the server setup process. Generate a keypair on each client, save the following config to <code>/etc/wireguard/wg0.conf</code> and replace the nessessary fields:</p>

<p><code></code>`conf
[Interface]
PrivateKey = &lt;Client Private Key from wg-private.key&gt;
Address = 10.0.0.2/32 # The fixed address of the client. Needs to be specified in the server config as well</p>

<p>[Peer]
PublicKey = &lt;Server Public key&gt;
AllowedIPs = 10.0.0.0/24 # Routes all traffic in this subnet to the server. If you want to tunnel all traffic through the wireguard connection, use 0.0.0.0/0 here instead
Endpoint = &lt;Public Server IP&gt;:51820
PersistentKeepalive = 25 # Optional. Will ping the server every 25 seconds to remain connected.
<code></code>`</p>

<p>On every client, run <code>wg-quick up wg0</code> to start the interface using the config at <code>/etc/wireguard/wg0.conf</code>.</p>

<p>This whole proccess might be easier on GUIs like Android or MacOS.</p>

<p>Now, try to ping your phone from your laptop:</p>

<p><code>
ping 10.0.0.3
PING 10.0.0.3 (10.0.0.3) 56(84) bytes of data.
64 bytes from 10.0.0.3: icmp_seq=1 ttl=64 time=5382 ms
64 bytes from 10.0.0.3: icmp_seq=2 ttl=64 time=4364 ms
</code></p>

<h3>References</h3>

<ul><li><a href="https://www.wireguard.com/">Official Documentation</a></li><li><a href="https://www.stavros.io/posts/how-to-configure-wireguard/">https://www.stavros.io/posts/how-to-configure-wireguard/</a></li></ul>]]>
            </description>
        </item>
    </channel>
</rss>