<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>
            garrit.xyz
        </title>
        <link>
            https://garrit.xyz
        </link>
        <description>
            Garrit Franke
        </description>
        <language>
            en
        </language>
        <lastBuildDate>
            Fri, 28 Apr 2023 00:00:00 +0000
        </lastBuildDate>
        <item>
            <title>
                Serverless Framework Retrospective
            </title>
            <guid>
                https://garrit.xyz/posts/2023-04-28-serverless-framework-retrospective
            </guid>
            <link>
                https://garrit.xyz/posts/2023-04-28-serverless-framework-retrospective?utm_source=rss
            </link>
            <pubDate>
                Fri, 28 Apr 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>A current project requires the infrastructure to be highly scalable. It&#39;s expected that &gt; 50k Users hit the platform within a five minute period. Regular ECS containers take about one minute to scale up. That just won&#39;t cut it. I decided to go all in on the <a href="https://www.serverless.com/">serverless</a> framework on AWS. Here&#39;s how it went.</p>

<h3>Setup</h3>

<p>Setting up a serverless application was a breeze. You create a config file and use their CLI to deploy the app.</p>

<h3>The rest of the infrastructure</h3>

<p>I decided to define the rest of the infrastructure (VPC, DB, cache, ...) in Terraform. But, since I wasn&#39;t familiar with how the Serverless Framework worked, I struggled to draw the line between what serverless should handle vs. what the rest of the infrastructure (Terraform) should provide. In a more traditional deployment workflow, you might let the CI deploy a container image to ECR and point the ECS service to that new image.</p>

<p>I chose to let Serverless deploy the entire app through CI and build the rest of the infrastructure around it. The problem with this approach is that we lose fine-grained control over what&#39;s deployed where, which leads to a lot of permission errors.</p>

<p>In retrospect, I should&#39;ve probably chosen the location of the S3 archive as the deployment target for the CI, and then point the lambda function to the location of the new artifact. This defeats the purpose of the framework, but it gives you a lot more control over your infrastructure. Once the next project comes along, I&#39;ll probably go that route instead.</p>

<h3>Permissions</h3>

<p>Serverless suggests to use admin permissions for deployments, and I see where they&#39;re coming from. Managing permissions in this framework is an absolute mess. Here&#39;s what the average deployment workflow looks like, if you want to use fine grained permissions:</p>

<ol><li>Wait for CloudFormation to roll back changes (~2 minutes)</li><li>Update IAM role</li><li>Deploy Serverless App</li><li>If there&#39;s an error, go to 1</li></ol>

<p>Thankfully, some people have already gone through the process of figuring this out. <a href="https://serverlessfirst.com/create-iam-deployer-roles-serverless-app/#determining-deploy-time-permissions">Here&#39;s</a> a great guide with a starting point of the needed permissions.</p>

<h3>Conclusion</h3>

<p>Using the serverless framework is a solid choice if you just want to throw an app out there. Unfortunately the app I was deploying isn&#39;t &quot;just&quot; a dynamic website. The next time I&#39;m building a serverless application it&#39;s probably not going to be with the Serverless Framework, though I learned a lot about serverless applications in general.</p>

<hr/>

<p>This is post 067 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Designing resilient cloud infrastructure
            </title>
            <guid>
                https://garrit.xyz/posts/2023-03-30-designing-resilient-cloud-infrastructure
            </guid>
            <link>
                https://garrit.xyz/posts/2023-03-30-designing-resilient-cloud-infrastructure?utm_source=rss
            </link>
            <pubDate>
                Thu, 30 Mar 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>As mentioned in a <a href="/posts/2023-03-16-terraform-project-learnings">previous post</a>, I&#39;m currently finishing up building my first cloud infrastructure on AWS for a client at work. During the development, I learned a lot about designing components to be resilient and scalable. Here are some key takeaways.</p>

<p>One of the most critical components of a resilient infrastructure is redundancy. On AWS, you place your components inside a &quot;region&quot;. This could be <code>eu-central-1</code> (Frankfurt) or <code>us-east-1</code> (North Virgina), etc. To further reduce the risk of an outage, each region is divided into multiple <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html">Availability Zones</a> (AZs). The AZs of a region are usually located some distance apart from each other. In case of a flood, a fire or a bomb detonating near one AZ, the other AZs should in most cases still be intact. You should have at least two, preferably three replicas of each component across multiple availability zones in a region. By having replicas of your components in different availability zones, you reduce the risk of downtime caused by an outage in a single availability zone.</p>

<p>Another way to ensure scalability and resilience for your database is to use <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html">Aurora Serverless v2</a>. This database service is specifically designed for scalable, on-demand, and cost-effective performance. The database scales itself up or down based on the workload, which allows you to automatically and dynamically adjust the database capacity to meet the demand of your application, ensuring that your application is responsive and performs well without the need for manual intervention. Adding Serverless instances to an existing RDS cluster is also a seemless proccess.</p>

<p>In addition to switching to Aurora Serverless v2, using read replicas for cache and database in a separate availability zone can act as a hot standby without extra configuration. Keep in mind that read replicas are only utilized by explicitly using the read-only endpoint of a cluster. But even if you&#39;re only using the &quot;main&quot; cluster endpoint (and therefore just the primary instance), a read replica can promote itself to the primary instance in case of a fail over, which drastically reduces downtime.</p>

<p>When using Amazon Elastic Container Service (ECS), use Fargate as opposed to EC2 instances. Fargate is a serverless compute engine for containers that allows you to run containers without having to manage the underlying infrastructure. It smartly locates instances across availability zones, ensuring that your application is always available.</p>

<p>In conclusion, you should always ensure that there are more than one instance of a component in your infrastructure. There are also services on AWS that abstract away the physical infrastructure (Fargate, S3, Lambda) and use a multi-AZ pattern by default.</p>

<hr/>

<p>This is post 061 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Terraform project learnings
            </title>
            <guid>
                https://garrit.xyz/posts/2023-03-16-terraform-project-learnings
            </guid>
            <link>
                https://garrit.xyz/posts/2023-03-16-terraform-project-learnings?utm_source=rss
            </link>
            <pubDate>
                Thu, 16 Mar 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I just finished my first ever infrastructure project for a client. My Terraform skills are good enough to be dangerous, but during the development of this project I learned a lot that I would do differently next time.</p>

<h2>Project structure</h2>

<p>Having worked with semi-professional Terraform code before, I applied what I knew to my new project. That was mainly that we have a shared base and an overlay directory for each environment. I went with a single Terraform module for the shared infrastructure, and variables for each environment. Naively, roughly every service had their own file.</p>

<p><code>
.
├── modules
│   └── infrastructure
│       ├── alb.tf
│       ├── cache.tf
│       ├── database.tf
│       ├── dns.tf
│       ├── ecr.tf
│       ├── ecs.tf
│       ├── iam.tf
│       ├── logs.tf
│       ├── main.tf
│       ├── network.tf
│       ├── secrets.tf
│       ├── security.tf
│       ├── ssl.tf
│       ├── state.tf
│       └── variables.tf
├── production
│   ├── main.tf
│   └── secrets.tf
└── staging
    ├── main.tf
    └── secrets.tf
</code></p>

<p>This works very well, but I already started running into issues extending this setup. For my next project, I would probably find individual components and turn them into smaller reusable submodules. If I were to rewrite the project above, I would probably structure it like this (not a complete project, but I think you get the idea):</p>

<p><code>
.
├── modules
│   └── infrastructure
│       ├── main.tf
│       ├── modules
│       │   ├── database
│       │   │   ├── iam.tf
│       │   │   ├── logs.tf
│       │   │   ├── main.tf
│       │   │   ├── outputs.tf
│       │   │   ├── rds.tf
│       │   │   └── variables.tf
│       │   ├── loadbalancer
│       │   │   ├── alb.tf
│       │   │   ├── logs.tf
│       │   │   ├── main.tf
│       │   │   ├── outputs.tf
│       │   │   └── variables.tf
│       │   ├── network
│       │   │   ├── dns.tf
│       │   │   ├── logs.tf
│       │   │   ├── main.tf
│       │   │   ├── outputs.tf
│       │   │   ├── ssl.tf
│       │   │   ├── variables.tf
│       │   │   └── vpc.tf
│       │   ├── service
│       │   │   ├── ecr.tf
│       │   │   ├── ecs.tf
│       │   │   ├── iam.tf
│       │   │   ├── logs.tf
│       │   │   ├── main.tf
│       │   │   ├── outputs.tf
│       │   │   └── variables.tf
│       │   └── state
│       │       ├── locks.tf
│       │       ├── main.tf
│       │       ├── outputs.tf
│       │       ├── s3.tf
│       │       └── variables.tf
│       ├── main.tf
│       ├── outputs.tf
│       └── variables.tf
├── production
│   ├── main.tf
│   └── secrets.tf
└── staging
    ├── main.tf
    └── secrets.tf
</code></p>

<h2>Secrets</h2>

<p>I decided to use <a href="https://github.com/AGWA/git-crypt">git-crypt</a> to manage secrets, but that was only before I learned about <a href="https://github.com/mozilla/sops">SOPS</a>. It&#39;s too late to migrate now, but if I could, I would choose SOPS for secrets any day of the week for upcoming projects. It even has a <a href="https://registry.terraform.io/providers/carlpett/sops/latest/docs">Terraform provider</a>, so there&#39;s no excuse not to use it. ;)</p>

<h2>Conclusion</h2>

<p>Overall I&#39;m pretty happy with how the project turned out, but there are some things that I learned during this project that will pay off later.</p>

<hr/>

<p>This is post 057 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Debugging ECS Tasks
            </title>
            <guid>
                https://garrit.xyz/posts/2023-03-10-debugging-ecs-tasks
            </guid>
            <link>
                https://garrit.xyz/posts/2023-03-10-debugging-ecs-tasks?utm_source=rss
            </link>
            <pubDate>
                Fri, 10 Mar 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I just had to debug an application on AWS ECS. The whole procedure is documented in more detail in the <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-exec.html">documentation</a>, but I think it&#39;s beneficial (both for my future self and hopefully to someone out there) to write down the proccess in my own words.</p>

<p>First of all, you need access to the cluster via the <a href="https://aws.amazon.com/de/cli/">CLI</a>. In addition to the CLI, you need the <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html">AWS Session Manager plugin for the CLI</a>. If you&#39;re on MacOS, you can install that via <a href="https://formulae.brew.sh/cask/session-manager-plugin">Homebrew</a>:</p>

<p><code>
brew install --cask session-manager-plugin
</code></p>

<p>Next, you need to allow the task you want to debug to be able to execute commands. Since I&#39;m using Terraform, this was just a matter of adding the <a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/ecs_service#enable_execute_command"><code>enable_execute_command</code></a> attribute to the service:</p>

<p><code>tf
resource &quot;aws_ecs_service&quot; &quot;my_service&quot; {
  name            = &quot;my-service&quot;
  cluster         = aws_ecs_cluster.my_cluster.id
  task_definition = aws_ecs_task_definition.my_task_definition.id
  desired_count   = var.app_count
  launch_type     = &quot;FARGATE&quot;
  enable_execute_command = true # TODO: Disable after debugging
}
</code></p>

<p>You may also need specify an execution role in the task definition:</p>

<p><code>tf
resource &quot;aws_ecs_task_definition&quot; &quot;my_task_definition&quot; {
  family              = &quot;my-task&quot;
  task_role_arn       = aws_iam_role.ecs_task_execution_role.arn
  execution_role_arn  = aws_iam_role.ecs_task_execution_role.arn  # &lt;-- Add this
}
</code></p>

<p>Make sure that this role has the correct access rights. There&#39;s a nice <a href="https://aws.amazon.com/de/premiumsupport/knowledge-center/ecs-error-execute-command/">troubleshooting guide</a> going over the required permissions.</p>

<p>If you had to do some modifications, make sure to roll out a new deployment with the fresh settings:</p>

<p><code>
aws ecs update-service --cluster my-cluster --service my-service --force-new-deployment
</code></p>

<p>Now, you should be able to issue commands against any running container!</p>

<p><code>
aws ecs execute-command --cluster westfalen --task &lt;task-id-or-arn&gt; --container my-container --interactive --command=&quot;/bin/sh&quot;
</code></p>

<p>I hope this helps!</p>

<hr/>

<p>This is post 055 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                The fundamental difference between Terraform and Kubernetes
            </title>
            <guid>
                https://garrit.xyz/posts/2023-03-08-terraform-and-kubernetes-are-fundamentally-different
            </guid>
            <link>
                https://garrit.xyz/posts/2023-03-08-terraform-and-kubernetes-are-fundamentally-different?utm_source=rss
            </link>
            <pubDate>
                Wed, 08 Mar 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>On the surface, Infrastructure as Code tools like <a href="https://www.terraform.io/">Terraform</a> or <a href="https://aws.amazon.com/de/cloudformation/">CloudFormation</a> may seem to behave similar to <a href="https://kubernetes.io/">Kubernetes</a> YAMLs, but they are in fact fundamentally different approaches to cloud infrastructure.</p>

<p>Terraform tries to provide a declarative way to express imperative actions. If you tell Terraform that you need an EC2 instance, it will notice that no such resource exists and instruct the AWS API to create one. If you don&#39;t need the instance anymore and remove the resource definition from your code, Terraform will also pick that up and instruct the AWS API to delete the instance. This works well in most cases, but every once in a while the declarative state may get out of sync with the real world, resulting in errors that are hard to debug and resolve.</p>

<p>Kubernetes on the other hand is a fully declarative system. In a <a href="/posts/2022-09-22-kubernetes-is-a-domain-specific-database">previous post</a> I touched on how Kubernetes constantly compares the <em>desired</em> state with the <em>actual</em> state of the resources and tries to match the two. Although it is theoretically possible to issue imperative actions, Kubernetes is built from the ground up to be declarative.</p>

<hr/>

<p>This is post 054 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Pods vs. Containers
            </title>
            <guid>
                https://garrit.xyz/posts/2023-03-04-pods-vs.-containers
            </guid>
            <link>
                https://garrit.xyz/posts/2023-03-04-pods-vs.-containers?utm_source=rss
            </link>
            <pubDate>
                Sat, 04 Mar 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>In Kubernetes, pods and containers are often confused. I found a <a href="https://iximiuz.com/en/posts/containers-vs-pods/">great article</a> going over the differences of the two terms.</p>

<blockquote><p>Containers and Pods are alike. Under the hood, they heavily rely on Linux namespaces and cgroups. However, Pods aren&#39;t just groups of containers. A Pod is a self-sufficient higher-level construct. All pod&#39;s containers run on the same machine (cluster node), their lifecycle is synchronized, and mutual isolation is weakened to simplify the inter-container communication. This makes Pods much closer to traditional VMs, <a href="https://www.mirantis.com/blog/multi-container-pods-and-container-communication-in-kubernetes/">bringing back the familiar deployment patterns like sidecar or reverse proxy</a>.</p></blockquote>

<p>In my own words: Containers are made up of Linux namespaces and cgroups. Pods can be thought of as a cgroup of cgroups (though not really), mimicing the behavior of a virtual machine that runs multiple containers with a synchronized lifecycle. The containers in a pod are losely isolated, making it easy to communicate between each other. Containers in a pod can however set individual resource requests, enabled by Linux namespaces.</p>

<p>I&#39;d highly encourage you to check out <a href="https://iximiuz.com/en/posts/containers-vs-pods/">the original article</a> if you want to learn more about this topic.</p>

<hr/>

<p>This is post 053 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Notes on containerizing PHP applications
            </title>
            <guid>
                https://garrit.xyz/posts/2023-03-03-notes-on-containerizing-php-applications
            </guid>
            <link>
                https://garrit.xyz/posts/2023-03-03-notes-on-containerizing-php-applications?utm_source=rss
            </link>
            <pubDate>
                Fri, 03 Mar 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I was recently tasked with building a rudimentary infrastructure for a PHP application. Coming from a Node.js-driven world where every human and their grandmother has a blog post about containerizing your application, it was very interesting to see where PHP differs to other applications.</p>

<p>One major gotcha for me was that PHP code is executed on <strong>request-time</strong>, meaning a new process is spawned for each incoming request. Most other languages have dedicated runtimes that handle incoming requests. This unique approach is very flexible and scalable, but it comes with the implication that there is a <strong>separate webserver</strong> that calls into the PHP interpreter when it needs to.</p>

<p>In Node.js (and most other languages), you can &quot;just run the app&quot;, as demonstrated by this Dockerfile:</p>

<p><code></code>`dockerfile
FROM node:18.14.2-alpine3.17 AS build</p>

<p>WORKDIR /usr/src/app</p>

<p>COPY package*.json ./</p>

<p>RUN npm ci</p>

<p>COPY . .</p>

<p>EXPOSE 3000</p>

<p>CMD [ &quot;node&quot;, &quot;server.js&quot; ]
<code></code>`</p>

<p>PHP on the other side is rarely used on its own. Most of the time, it needs a webserver alongside it:</p>

<p><code></code>`dockerfile
FROM php:8.1-apache-bullseye</p>

<h1>&lt;snip&gt;</h1>

<p>COPY . /var/www/html
WORKDIR /var/www/html</p>

<h1>&lt;snip&gt;</h1>

<p><code></code>`</p>

<p>As you can see, I&#39;m using the official PHP docker image. The PHP maintainers know that adding a webserver alongside PHP is a very common pattern, so most of the variants of the image ship with a webserver. In this example I&#39;m using Apache, but we might as well use NGINX or some other webserver. There&#39;s also the option to use <a href="https://www.php.net/manual/de/install.fpm.php">FPM</a> as a FastCGI implementation and a webserver in a <strong>separate</strong> container.</p>

<p>Grasping this took me some time, but after it clicked it made many things a lot clearer.</p>

<h2>More complete Dockerfile example</h2>

<p>The Dockerfile above is meant to demonstrate how PHP applications differ from other languages. The following is a more complete example you can use to containerize your PHP application. In this case it’s a Laravel app, so your mileage may vary.</p>

<p><code></code>`dockerfile
FROM php:8.1-apache-bullseye</p>

<p>RUN apt-get clean &amp;&amp; \
    apt-get update &amp;&amp; \
    apt-get install --fix-missing -y \
        zip &amp;&amp; \
    docker-php-ext-install \
        pdo \
        pdo_mysql \
        bcmath</p>

<p>COPY --from=composer:2 /usr/bin/composer /usr/bin/composer</p>

<p>COPY . /var/www/html
WORKDIR /var/www/html</p>

<p>ENV APACHE<em>DOCUMENT</em>ROOT /var/www/html/public</p>

<p>RUN composer install --no-dev --optimize-autoloader --no-interaction &amp;&amp; \
    sed -ri -e &#39;s!/var/www/html!${APACHE<em>DOCUMENT</em>ROOT}!g&#39; /etc/apache2/sites-available/*.conf &amp;&amp; \
    sed -ri -e &#39;s!/var/www/!${APACHE<em>DOCUMENT</em>ROOT}!g&#39; /etc/apache2/apache2.conf /etc/apache2/conf-available/*.conf &amp;&amp; \
    php artisan config:cache &amp;&amp; \
    php artisan view:cache &amp;&amp; \
    php artisan route:cache &amp;&amp; \
    php artisan storage:link &amp;&amp; \
    chmod 777 -R /var/www/html/storage/ &amp;&amp; \
    chown -R www-data:www-data /var/www/ &amp;&amp; \
    a2enmod rewrite
<code></code>`</p>

<hr/>

<p>This is post 052 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                What's next for modern infrastructure?
            </title>
            <guid>
                https://garrit.xyz/posts/2023-02-21-what's-next-for-modern-infrastructure
            </guid>
            <link>
                https://garrit.xyz/posts/2023-02-21-what's-next-for-modern-infrastructure?utm_source=rss
            </link>
            <pubDate>
                Tue, 21 Feb 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>Modern infrastructure is incredibly complex. I identified 4 main &quot;levels&quot; of infrastructure abstraction:</p>

<h2>Level 1: A website on a server</h2>

<p>This is the most straight forward way to host a website. A webserver hosted on bare metal or a VM.</p>

<h2>Level 2: Multiple servers behind a load balancer</h2>

<p>At this stage, you start treating servers as cattle rather than pets. Servers may be spun up and down at will without influencing the availability of the application.</p>

<h2>Level 3: An orchestrated cluster of servers</h2>

<p>Instead of a server serving a specific purpose (e.g. webserver, DB server, etc.), a server becomes a worker for arbitrary workloads (see Kubernetes, ECS).</p>

<h2>Level 4: Multicluster service mesh</h2>

<p>If an organization manages multiple clusters (e.g. multiple application teams), they can be tied together into a <a href="https://istio.io/latest/docs/reference/glossary/#service-mesh">service mesh</a> to better optimize communication and observability.</p>

<h2>Level 5: ???</h2>

<p>History shows that we never stop abstracting. Multicluster service meshes are about the most abstract concept many people (including myself) can comprehend, but I doubt that this is the end of this journey. So, what&#39;s next for modern infrastructure?</p>

<p>This is post 049 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                What problem does Kubernetes solve?
            </title>
            <guid>
                https://garrit.xyz/posts/2023-02-20-what-problem-does-kubernetes-solve
            </guid>
            <link>
                https://garrit.xyz/posts/2023-02-20-what-problem-does-kubernetes-solve?utm_source=rss
            </link>
            <pubDate>
                Mon, 20 Feb 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>This is a common question that many people (including me) ask themselves.</p>

<p>I recently came across a great <a href="https://blog.adamchalmers.com/kubernetes-problems/">post</a> which explains the problem really well:</p>

<blockquote><p>Kubernetes exists to solve one problem: how do I run m containers across n servers?</p></blockquote>

<p>The post also nails the answer to <strong>how</strong> Kubernetes solves this problem:</p>

<blockquote><p>It&#39;s a big abstract virtual computer, with its own virtual IP stack, networks, disk, RAM and CPU. It lets you deploy containers as if you were deploying them on one machine that didn&#39;t run anything else. Clusters abstract over the various physical machines that run the cluster.</p></blockquote>

<p>I&#39;d highly encourage you to read through the article if you want to learn more about why Kubernetes exists.</p>

<p>This is post 048 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Reselling Kubernetes
            </title>
            <guid>
                https://garrit.xyz/posts/2022-11-03-reselling-managed-kubernetes
            </guid>
            <link>
                https://garrit.xyz/posts/2022-11-03-reselling-managed-kubernetes?utm_source=rss
            </link>
            <pubDate>
                Thu, 03 Nov 2022 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I&#39;m currently working on a side project involving reselling Kubernetes clusters.
What I discovered is that it&#39;s impossible to resell <em>managed</em> Kubernetes, as in
<a href="https://aws.amazon.com/de/eks/">EKS</a> and
<a href="https://cloud.google.com/kubernetes-engine/">GKE</a>.</p>

<p>The only possible scenario where reselling Kubernetes to your and the
end-customers advantage is to manage the nodes yourself. The reason is the
scaling of cost per CPU.</p>

<p>When renting VMs, the price per CPU often varies for the size of the machine. 
This leaves the reseller flexibility in the choice of resources. To give you an
example, here is the pricing for virtual machines at
<a href="https://www.hetzner.com/">Hetzner</a>, and the price per CPU:</p>

<p>| Product         | Number of vCPUs    | Price (in €/month) | Price per vCPU (in €/month) |
|-----------------|--------------------|--------------------|-----------------------------|
| CX11            | 1                  | 4.51               | 4.51                        |
| CPX11           | 2                  | 5.18               | 2.59                        |
| CPX21           | 3                  | 8.98               | 2.99                        |
| CPX31           | 4                  | 16.18              | 4.05                        |
| CPX41           | 8                  | 29.99              | 3.75                        |
| CPX51           | 16                 | 65.33              | 4.08                        |</p>

<p>When comparing this to a managed Kubernetes product like
<a href="https://www.civo.com">CIVO</a>, we see that the price per CPU stays constant:</p>

<p>| Product         | Number of vCPUs    | Price (in €/month) | Price per vCPU (in €/month) |
|-----------------|--------------------|--------------------|-----------------------------|
| Extra Small     | 1                  | 5                  | 5                           |
| Small           | 2                  | 10                 | 5                           |
| Medium          | 4                  | 20                 | 5                           |
| Large           | 8                  | 40                 | 5                           |</p>

<p>This pricing model is nice and predictable for the customer, but it makes it
impossible to justify a resell product. If CIVO charges 5€/month per vCPU, we
would need to charge extra to be profitable, which in turn overcuts the
competition.</p>

<p>When choosing Hetzner (or any other platform offering VMs), we are still able to
undercut the competition and even optimize how the resources are laid out on
the nodes. The obvious downside of course being that we have to manage the
clusters ourselves.</p>

<h2>Share your thoughts</h2>

<p>Reselling Kubernetes is tricky. I&#39;m currently sketching out ideas for an
alternative way to sell Kubernetes hosting at an ultra cheap price. The project
is still in its infancy but if you&#39;re interested, you&#39;re more than welcome to
share your thoughts in our <a href="https://matrix.to/#/!cTXkqtlnbHScIxnlqO:matrix.org?via=matrix.org&amp;via=envs.net">Matrix
Room</a>!</p>

<p>This is post 041 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
    </channel>
</rss>