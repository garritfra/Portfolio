<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>
            garrit.xyz
        </title>
        <link>
            https://garrit.xyz
        </link>
        <description>
            Garrit Franke
        </description>
        <language>
            en
        </language>
        <lastBuildDate>
            Thu, 27 Apr 2023 00:00:00 +0000
        </lastBuildDate>
        <item>
            <title>
                Migrating Homeassistant from SD to SSD
            </title>
            <link>
                https://garrit.xyz/posts/2023-04-27-migrating-homeassistant-from-sd-to-ssd?utm_source=rss
            </link>
            <pubDate>
                Thu, 27 Apr 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I finally got frustrated with the performance of my Raspberry Pi 4 running Homeassistant on a SD card, so I went ahead and got an SSD.</p>

<p>The migration was <strong>very</strong> easy:</p>

<ol><li>Create and download a full backup through the UI</li><li>Flash Homeassistant onto the SSD</li><li>Remove the SD card and plug the SSD into a USB 3.0 port of the Pi</li><li>Boot</li><li>Go through the onboarding procedure</li><li>Restore Backup</li><li>Profit</li></ol>

<p>It worked like a charm! The speed has improved A LOT, and everything was set up as it should be. </p>

<p>...Until we turned on the lights in the livingroom. My ZigBee-dongle, plugged into another USB port, wasn&#39;t able to communicate with the devices on the network.</p>

<p>After some digging around, I came across several threads stating that an SSD over USB 3.0 apparently creates a lot of interference to surrounding hardware, including my ZigBee dongle. The fix was simple: either get an extension port for the dongle, or plug the SSD into a USB 2.0 port of the Pi. Since I didn&#39;t have an extension cord to get the dongle far away enough from the SSD, I went with the latter option for now. And that fixed it! The performance was much worse, but still better than the SD I used before. My next step will be to grab an extension cord from my parents. I&#39;m sure they won&#39;t mind.</p>

<p>I hope this helps!</p>

<hr/>

<p>This is post 066 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Instant dark theme
            </title>
            <link>
                https://garrit.xyz/posts/2023-04-12-instant-dark-theme?utm_source=rss
            </link>
            <pubDate>
                Wed, 12 Apr 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>Thanks to <a href="https://jacksonchen666.com/">Jacksons</a> <a href="https://github.com/garritfra/darktheme.club/pull/79">update to darktheme.club</a>, I just came across a neat little <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/color-scheme">CSS property</a> that turns a mostly CSS-free document into a pleasantly dark site:</p>

<p><code>css
:root {
  color-scheme: light dark;
}
</code></p>

<p>This will adjust all elements on the page to the color scheme preferred by the user - without any other custom styles! 🤯 It is also <a href="https://caniuse.com/mdn-css_properties_color-scheme">widely supported</a> by browsers.</p>

<p>I&#39;ve always been quite dependent on CSS-frameworks for any project I&#39;m starting. Going forward, I&#39;d be interested to see how framework-less sites would feel using this property. If all else fails, there&#39;s always the awesome <a href="https://simplecss.org/">simple.css</a> library, which you can slap on top of a raw document to make it pretty (and dark, if preferred) without using custom classes.</p>

<hr/>

<p>This is post 064 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Designing resilient cloud infrastructure
            </title>
            <link>
                https://garrit.xyz/posts/2023-03-30-designing-resilient-cloud-infrastructure?utm_source=rss
            </link>
            <pubDate>
                Thu, 30 Mar 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>As mentioned in a <a href="/posts/2023-03-16-terraform-project-learnings">previous post</a>, I&#39;m currently finishing up building my first cloud infrastructure on AWS for a client at work. During the development, I learned a lot about designing components to be resilient and scalable. Here are some key takeaways.</p>

<p>One of the most critical components of a resilient infrastructure is redundancy. On AWS, you place your components inside a &quot;region&quot;. This could be <code>eu-central-1</code> (Frankfurt) or <code>us-east-1</code> (North Virgina), etc. To further reduce the risk of an outage, each region is divided into multiple <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html">Availability Zones</a> (AZs). The AZs of a region are usually located some distance apart from each other. In case of a flood, a fire or a bomb detonating near one AZ, the other AZs should in most cases still be intact. You should have at least two, preferably three replicas of each component across multiple availability zones in a region. By having replicas of your components in different availability zones, you reduce the risk of downtime caused by an outage in a single availability zone.</p>

<p>Another way to ensure scalability and resilience for your database is to use <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html">Aurora Serverless v2</a>. This database service is specifically designed for scalable, on-demand, and cost-effective performance. The database scales itself up or down based on the workload, which allows you to automatically and dynamically adjust the database capacity to meet the demand of your application, ensuring that your application is responsive and performs well without the need for manual intervention. Adding Serverless instances to an existing RDS cluster is also a seemless proccess.</p>

<p>In addition to switching to Aurora Serverless v2, using read replicas for cache and database in a separate availability zone can act as a hot standby without extra configuration. Keep in mind that read replicas are only utilized by explicitly using the read-only endpoint of a cluster. But even if you&#39;re only using the &quot;main&quot; cluster endpoint (and therefore just the primary instance), a read replica can promote itself to the primary instance in case of a fail over, which drastically reduces downtime.</p>

<p>When using Amazon Elastic Container Service (ECS), use Fargate as opposed to EC2 instances. Fargate is a serverless compute engine for containers that allows you to run containers without having to manage the underlying infrastructure. It smartly locates instances across availability zones, ensuring that your application is always available.</p>

<p>In conclusion, you should always ensure that there are more than one instance of a component in your infrastructure. There are also services on AWS that abstract away the physical infrastructure (Fargate, S3, Lambda) and use a multi-AZ pattern by default.</p>

<hr/>

<p>This is post 061 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Fullscreen Terminals in VSCode
            </title>
            <link>
                https://garrit.xyz/posts/2023-03-23-fullscreen-terminals-in-vscode?utm_source=rss
            </link>
            <pubDate>
                Thu, 23 Mar 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I often find myself using a &quot;real&quot; terminal alongside my VSCode setup, because for some tasks the built-in terminal, due to its small size, is quite flimsy to use. <em>But</em>! I just found out there&#39;s a a way to switch the terminal into fullscreen mode, using the &quot;View: Toggle Maximized Panel&quot; command.</p>

<p>You can bind it to a shortcut, which makes switching between editor and terminal a breeze! Simply add this to your <code>keybindings.json</code> (also accessible via the <a href="https://code.visualstudio.com/docs/getstarted/userinterface#_command-palette">command palette</a>):</p>

<p><code>
    {
        &quot;key&quot;: &quot;cmd+alt+m&quot;,
        &quot;command&quot;: &quot;workbench.action.toggleMaximizedPanel&quot;
    }
</code></p>

<h3>References</h3>

<ul><li><a href="https://stackoverflow.com/a/48512128/9046809">Original StackOverflow answer</a></li></ul>

<hr/>

<p>This is post 059 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Debugging ECS Tasks
            </title>
            <link>
                https://garrit.xyz/posts/2023-03-10-debugging-ecs-tasks?utm_source=rss
            </link>
            <pubDate>
                Fri, 10 Mar 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I just had to debug an application on AWS ECS. The whole procedure is documented in more detail in the <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-exec.html">documentation</a>, but I think it&#39;s beneficial (both for my future self and hopefully to someone out there) to write down the proccess in my own words.</p>

<p>First of all, you need access to the cluster via the <a href="https://aws.amazon.com/de/cli/">CLI</a>. In addition to the CLI, you need the <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html">AWS Session Manager plugin for the CLI</a>. If you&#39;re on MacOS, you can install that via <a href="https://formulae.brew.sh/cask/session-manager-plugin">Homebrew</a>:</p>

<p><code>
brew install --cask session-manager-plugin
</code></p>

<p>Next, you need to allow the task you want to debug to be able to execute commands. Since I&#39;m using Terraform, this was just a matter of adding the <a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/ecs_service#enable_execute_command"><code>enable_execute_command</code></a> attribute to the service:</p>

<p><code>tf
resource &quot;aws_ecs_service&quot; &quot;my_service&quot; {
  name            = &quot;my-service&quot;
  cluster         = aws_ecs_cluster.my_cluster.id
  task_definition = aws_ecs_task_definition.my_task_definition.id
  desired_count   = var.app_count
  launch_type     = &quot;FARGATE&quot;
  enable_execute_command = true # TODO: Disable after debugging
}
</code></p>

<p>You may also need specify an execution role in the task definition:</p>

<p><code>tf
resource &quot;aws_ecs_task_definition&quot; &quot;my_task_definition&quot; {
  family              = &quot;my-task&quot;
  task_role_arn       = aws_iam_role.ecs_task_execution_role.arn
  execution_role_arn  = aws_iam_role.ecs_task_execution_role.arn  # &lt;-- Add this
}
</code></p>

<p>Make sure that this role has the correct access rights. There&#39;s a nice <a href="https://aws.amazon.com/de/premiumsupport/knowledge-center/ecs-error-execute-command/">troubleshooting guide</a> going over the required permissions.</p>

<p>If you had to do some modifications, make sure to roll out a new deployment with the fresh settings:</p>

<p><code>
aws ecs update-service --cluster my-cluster --service my-service --force-new-deployment
</code></p>

<p>Now, you should be able to issue commands against any running container!</p>

<p><code>
aws ecs execute-command --cluster westfalen --task &lt;task-id-or-arn&gt; --container my-container --interactive --command=&quot;/bin/sh&quot;
</code></p>

<p>I hope this helps!</p>

<hr/>

<p>This is post 055 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Visual Distractions
            </title>
            <link>
                https://garrit.xyz/posts/2023-02-24-visual-distractions?utm_source=rss
            </link>
            <pubDate>
                Fri, 24 Feb 2023 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>Everywhere we look, we&#39;re bombarded with flashy symbols trying to grab our attention. This is even the case where we <strong>think</strong> that we&#39;re in control of what we&#39;re looking at. I made two simple changes that reduce visual distractions in my life.</p>

<h2>Android App Icons</h2>

<p>App icons play a serious role in how we interact with our phone. Over the years, there has been a constant battle for the most flashy icon on our home screen. But there&#39;s a cure: newer versions of Android <a href="https://www.lifewire.com/change-color-of-apps-on-android-phones-5213663">let you choose a color theme for apps that implement it</a>. It&#39;s by far not supported by every app out there, but in my case 90% of the app icons now have the same color. I feel way more comfortable looking at my phone, knowing that less things are trying to grab my attention right when I unlock my phone.</p>

<p>With this change, I found that I am more mindful about what app icon I tap on, since I was used to each icon having a different color. This makes it harder for my muscle memory to develop bad habits.</p>

<h2>RSS-Reader Favicons</h2>

<p>If you&#39;re using an RSS reader, chances are you&#39;re used to seeing a favicon next to the articles. I had the feeling that I was drawn more towards the favicon than the headline of the article, so I started looking for ways to disable favicons all together.</p>

<p>Miniflux provides a way to override the stylesheet of the feed in the settings. Simply append the following code-snippet and the favicons will be history:</p>

<p><code>css
.item-title img, .entry-website img {
  display: none;
}
</code></p>

<p>Of course every reader is different, so you might want to look into the documentation of your reader of choice.</p>

<h2>Conclusion</h2>

<p>These changes might seem insignificant, but I found that they made a huge difference in how I interact with my phone. The suggestions above might not apply to your life, but I&#39;d like to encourage you to keep an eye out for unnecessary visual distraction in your life. Try to avoid it as much as possible.</p>

<hr/>

<p>This is post 051 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                6 ways you can contribute to open knowledge right now
            </title>
            <link>
                https://garrit.xyz/posts/2022-12-05-contributing-to-open-source-knowledge?utm_source=rss
            </link>
            <pubDate>
                Mon, 05 Dec 2022 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I wrote the initial draft for this post a few months ago, traveling through
Norway in a rented campervan. While roaming the beautiful landscapes, I spent a
lot of time thinking. Reading books while traveling really is the best way to
find new inspiration.</p>

<p>On our trip, we wanted to try out an alternative to Google Maps. Most of the
OpenStreetMap-based apps lack important features, but we recently stumbled upon
<a href="https://www.magicearth.com/">MagicEarth</a>, which perfectly fills the void.
OpenStreetMap has been 95% accurate for us. Those last 5% are mostly less famous
hiking trails and attractions that could easily be filled in by people like you
and me. This inspired me to write this blog post, where I share six ways that
you can contribute to open source knowledge right now.</p>

<h2>OpenStreetMap</h2>

<p>As mentioned above, I spotted some minor inconsistencies in
<a href="https://openstreetmap.org">OpenStreetMap</a> while driving through Norway. We
tracked our hikes with an app that is able to export a GPX file, which can be
imported to OpenStreetMap to check if the trail matches (or if it is missing),
and took note of incorrect or sloppy roads/buildings. Back home, I plan to sit
down and fix up those issues.</p>

<p>But you don&#39;t have to be on a roadtrip to contribute to OpenStreetMap! Chances
are you know your local surroundings pretty well. Just navigate to your
neighborhood and see what could be improved. Maybe you know a public toilet, a
park or a secret road that is not shown on the map? As a matter of fact, my
private address was missing, so I added it via the editor. I can now use any of
the many OpenStreetMap-based apps to navigate home!</p>

<h2>Wikipedia (and other wikis)</h2>

<p>I often feel like I can&#39;t contribute much to the vast knowledge of Wikipedia.
<em>Other people are way smarter than me</em> and whatnot. But while you might not be
able to publish worthy edits to a well-known topic, you might know some things
that others haven&#39;t thought of. Is there an entry about your local town? Is
there an interesting member of your (past) family that others might want to read
about?</p>

<p>Of course, there are other wikis beside Wikipedia. Are you using a little-known
tool that has open source documentation in the form of a wiki? How can it be
improved?</p>

<h2>Observation</h2>

<p>You might have never heard of <a href="https://observation.org">observation.org</a>. It&#39;s
an open biodiversity- and nature-database. I just recently learned about them in
our local museum. They had a special exhibition about insects, and called out
for contributions to map out our local flora and fauna.</p>

<p>The idea is simple: snap a picture of an interesting looking insect or plant,
upload it using the website (or one of their apps) and create an &quot;observation&quot;.
Using this information, researchers will be able to understand the biodiversity
of your area. The information is free to use, and anyone can contribute!</p>

<h2>Wardriving</h2>

<p>Wardriving is a fun and useful way to contribute to open source knowledge. By
driving around with a device that can detect and record wireless networks, you
can help to map out the wireless coverage in your area. This information can be
used by researchers, network operators, and other interested parties to
understand the availability and quality of wireless networks.</p>

<p>One popular tool for wardriving is <a href="https://wigle.net/">WiGLE</a>. WiGLE allows you
to easily collect and share information about wireless networks, and contribute
to the global wireless map. To get started with WiGLE, you will need a device
that can detect and record wireless networks. This can be a smartphone, laptop,
or dedicated wardriving device. You will also need to download and install the
WiGLE app, and some basic knowledge of how to use it.</p>

<p>Once you have set up WiGLE, you can start driving around and mapping out the
wireless networks in your area. As you collect data, it will be automatically
uploaded to the WiGLE database, where it can be used by researchers and other
interested parties. Wardriving with WiGLE is a fun and easy way to help advance
scientific research and understanding.</p>

<h2>folding@home</h2>

<p>Another way to contribute to open source knowledge is to participate in the
<a href="https://foldingathome.org/">folding@home</a> project. folding@home is a
distributed computing project that uses the idle processing power of volunteers&#39;
computers to perform scientific calculations and simulations. These calculations
are used to study a wide range of topics, including protein folding, drug
design, and the origins of the universe.</p>

<p>By joining the folding@home network, you can help to advance scientific research
and discovery. The project is open to anyone, and you can participate using your
personal computer, laptop, or even your smartphone. All you need to do is
download and install the folding@home software, and then select the types of
calculations that you want to contribute to.</p>

<p>The folding@home project is a great way to put your idle computing power to good
use, and to contribute to the global effort to advance scientific knowledge. To
learn more, visit the <a href="https://foldingathome.org/">folding@home website</a>.</p>

<h2>Blog posts</h2>

<p>Writing a blog post is a fun and engaging way to contribute to open source
knowledge. You don&#39;t need to be a professional writer or have a formal writing
style. Just jot down some notes about a topic that you are passionate about, and
share your experiences and expertise with others.</p>

<p>Not only will you be helping others to learn from your experiences, but writing
a blog post can also be beneficial for yourself. Capturing your thoughts and
ideas in writing can help you to better understand and organize your own
knowledge. It can also be a great way to reflect on your experiences and to
learn from your successes and failures.</p>

<p>If you&#39;re interested in blogging, you might want to check out the
<a href="https://100daystooffload.com/">100DaysToOffload</a> project!</p>

<h1>Wrapping up</h1>

<p>As you can see, there are many ways that you can contribute to open source
knowledge, even if you don&#39;t have a lot of time or expertise. By participating
in projects like OpenStreetMap, Wikipedia, observation.org, and folding@home, or
by sharing your experiences and expertise through blog posts, you can make a
real difference in the community.</p>

<p>Why not give it a try? You might be surprised by how much you can learn and how
much you can help others. And who knows, you might even have some fun along the
way! Thanks for reading, and happy contributing!</p>

<p>This is post 044 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                A simple guestbook
            </title>
            <link>
                https://garrit.xyz/posts/2022-10-05-simple-guestbook?utm_source=rss
            </link>
            <pubDate>
                Wed, 05 Oct 2022 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<blockquote><p><strong>TL;DR</strong>: Click <a href="/guestbook">here</a> to view the guestbook.</p></blockquote>

<p>For a while now, I wanted to have a quick way to update the pages on my website.</p>

<p>GitHub has the
<a href="https://docs.github.com/en/get-started/using-github/keyboard-shortcuts#source-code-editing">&quot;.&quot;</a>
hotkey, which opens a web based editor for the file you&#39;re currently viewing.
This site now has this feature as well! To try it out, just hit <code>.</code>, and you&#39;ll
be redirected to the file editor for this page.</p>

<p>To see how I implemented this feature, you take a look at
<a href="https://github.com/garritfra/garrit.xyz/commit/658efa3a3ebfebebbf74d0eb6aae6c1cc9566516">this</a>
commit. It boils down to this snippet of code:</p>

<p><code></code><code>js
window.addEventListener(&quot;keypress&quot;, (e) =&gt; {
	if (e.key === &quot;.&quot;) {
		const baseUrl = &quot;https://github.com/garritfra/garrit.xyz/edit/main/content&quot;;
		const filePath = window.location.pathname;
		const url = </code>${baseUrl}${filePath}.md`;</p>

<pre><code>	window.location.href = url;
}</code></pre>

<p>});
<code></code>`</p>

<p>Pretty simple, huh?</p>

<p>Since this doesn&#39;t work on mobile devices, I also added <a href="https://github.com/garritfra/garrit.xyz/commit/8c374a8bc0b66192c454300489fee52e7299c9dd#diff-2cbafea0c9dff483ebab9ad670b1cdb3eb7aac552f9c161e42fee84c2efe3a69">a custom 404
page</a>
which also redirects to the editor, if the filepath ends with in <code>/edit</code>.</p>

<p>Let&#39;s have some fun and put this feature to use. I added a simple
<a href="/guestbook">guestbook</a> to this site, which is open to receive pull requests.
I&#39;d love to hear from you!</p>

<p>This is post 040 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Debugging Docker images
            </title>
            <link>
                https://garrit.xyz/posts/2022-09-30-debugging-docker-images?utm_source=rss
            </link>
            <pubDate>
                Fri, 30 Sep 2022 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>Docker builds images incrementally. Every line in a Dockerfile will generate a
new image that builds on top of the last one. This can be really handy if
something is not right in your build.</p>

<p>Since version 18.09 Docker has added a new backend for building images,
<a href="https://github.com/moby/buildkit#buildkit">buildkit</a>. Unfortunately, buildkit
does not let you view the IDs of the intermediate containers it uses under the
hood. To work around that, you can opt out of buildkit by running a build with
buildkit disabled:</p>

<p><code>sh
DOCKER_BUILDKIT=0 docker build --pull --rm -t myproject:latest .
</code></p>

<p>You should now see the IDs of the intermediate containers:</p>

<p><code>sh
Sending build context to Docker daemon  87.84MB
Step 1/16 : FROM node:16.15.1-alpine3.16 AS development
16.15.1-alpine3.16: Pulling from library/node
Digest: sha256:c785e617c8d7015190c0d41af52cc69be8a16e3d9eb7cb21f0bb58bcfca14d6b
Status: Image is up to date for node:16.15.1-alpine3.16
 ---&gt; e548f8c9983f
Step 2/16 : WORKDIR /usr/src/app
 ---&gt; Using cache
 ---&gt; 34e5c9bdb910
Step 3/16 : COPY package*.json ./
 ---&gt; Using cache
 ---&gt; 626e4ae998fc
Step 4/16 : RUN npm install glob rimraf
 ---&gt; Using cache
 ---&gt; 2d036b8354e0
Step 5/16 : RUN npm install
 ---&gt; Using cache
 ---&gt; 948709b4957f      &lt;-- HERE
Step 6/16 : COPY . .
...
</code></p>

<p>As mentioned, these IDs are valid docker images, so you can just launch them
and attach a shell like every other image:</p>

<p><code>sh
docker run -ti --rm 948709b4957f
</code></p>

<p>If you&#39;re not seeing a regular shell, but a Node.js REPL for example, this
might be because the <code>ENTRYPOINT</code> of that image was set to the binary of that
REPL. To work around that, you can override the entrypoint:</p>

<p><code>sh
docker run -ti --rm --entrypoint=/bin/sh 948709b4957f
</code></p>

<h2>When is this helpful?</h2>

<p>If your build fails at a particular step, you can attach a shell to the <strong>last
working</strong> step, inspect the filesystem, and execute the failing command manually.</p>

<p>That&#39;s all!</p>

<p>This is post 039 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                The only true answer to 'tabs vs spaces'
            </title>
            <link>
                https://garrit.xyz/posts/2022-06-29-the-only-true-answer-to-tabs-vs-spaces?utm_source=rss
            </link>
            <pubDate>
                Wed, 29 Jun 2022 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I recently dove into a new project at work. We&#39;re starting from a blank page,
so of course this classic question came up:</p>

<blockquote><p>&quot;So should we use tabs or spaces for our formatting?&quot;</p></blockquote>

<p>One of my teammates explained to us why the only logical answer to this is
&quot;Tabs&quot;, and you&#39;ll soon know why.</p>

<h2>The problem</h2>

<p>Most formatters, by default, use either two or four spaces for indentation by
default. The <a href="https://prettier.io/">Prettier</a> formatter does this, and it
somewhat became the norm for JavaScript projects. This has one huge downside
though: everyone on the team has to agree, or live with this standard.</p>

<p>Nowadays, almost all editors come with the ability to change the preferred
indentation settings, which will be overridden by the settings of the
formatter. I prefer an indentation of 4 spaces, which is reflected in all of my
code. If I&#39;m working on a project that uses an indentation of 2 spaces via
prettier, my preference will be overridden when formatting the code.</p>

<h2>Just use tabs</h2>

<p>The solution to this problem is simple: Create a <code>.editorconfig</code> file and set
the indentation style to tab, without a width:</p>

<p><code></code>`editorconfig
root = true</p>

<p>[*]
end<em>of</em>line = lf
charset = utf-8
indent_style = tab
<code></code>`</p>

<p>Almost all editors will be able to pick this file up and configure some
project-wide settings. If your editor is configured to use a indent width of 4,
this setting will be respected. If you&#39;re a maniac that indents their code with
8 spaces, you&#39;ll be pleased to see that you can finally use this style in your
code, without forcing anyone else to do as you do.</p>

<p>Even GitHub, GitLab and friends are able to respect this setting, giving
everyone the opportunity to view code in their preferred style.</p>

<p>I hope you now know why using a single tab of indentation makes the most sense if
you&#39;re working in a team. Let me know your thoughts!</p>

<p>This is post 035 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Postgres Docker Container Migration Cheat Sheet
            </title>
            <link>
                https://garrit.xyz/posts/2022-05-31-database-server-migration-cheat-sheet?utm_source=rss
            </link>
            <pubDate>
                Tue, 31 May 2022 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I just finished migrating a postgres database to a new host. To remember how to
do it next time, I&#39;m writing down the commands I used here.</p>

<p>I usually just shut down the database and then copy the local directory where
the volume was mounted onto the new host. This time though, I seemed to be
getting some I/O errors, so I had to do it the &quot;right&quot; way.</p>

<p>To be fair, this note is based on
<a href="https://www.netguru.com/blog/how-to-dump-and-restore-postgresql-database">this</a>
guide. I modified it to fit my workflow with docker.</p>

<h2>Creating a dump</h2>

<p>Log into the old host:</p>

<p><code>
ssh &lt;user&gt;@host
</code></p>

<p>Connect to the postgres-container:</p>

<p><code>
docker exec -ti myservice_db_1 /bin/bash
</code></p>

<p>Create a dump. You can name your dump as you wish - I&#39;m using dates to
distinguish multiple dumps:</p>

<p><code>
pg_dump -U db_user db_name &gt; db_name_20220531.sql
</code></p>

<p>Copy the dump to the host machine:</p>

<p><code>
docker cp myservice_db_1:/db_name_20220531.sql ~/
</code></p>

<h2>Moving the dump to the new host</h2>

<p>The easiest way to get the dump off of the old server and onto the new one is to
use your local machine as a middleman.</p>

<p>First, download the dump to your machine:</p>

<p><code>
scp &lt;user&gt;@&lt;host&gt;:~/db_name_20220531.sql .
</code></p>

<p>Then, do the same thing but reversed, with the new host:</p>

<p><code>
scp ./db_name_20220531.sql &lt;user&gt;@&lt;host&gt;:~/
</code></p>

<h2>Restoring the dump</h2>

<p>First, connect to the new host:</p>

<p><code>
ssh &lt;user&gt;@&lt;host&gt;
</code></p>

<p>Assuming the docker service is already running on the new host, attach to the
db-container, just like above:</p>

<p><code>
docker exec -ti myservice_db_1 /bin/bash
</code></p>

<p>This time, we have to do some fiddling on the database, so attach a session to
postgres using their cli:</p>

<p><code>
psql -U my_user
</code></p>

<p>Before &quot;resetting&quot; the existing DB to apply the dump, we have to connect to
another database. The <code>postgres</code> DB is always there, so you can use that.</p>

<p><code>
\c postgres
</code></p>

<p>Now, we drop the existing DB and re-add it:</p>

<p><code>sql
drop database database_name;
create database database_name with owner your_user_name;
</code></p>

<p>And now, the moment you&#39;ve been waiting for! Leave the psql-session and apply
the dump:</p>

<p><code>
psql -U db_user db_name &lt; db_name_20220531.sql
</code></p>

<p>That&#39;s all! You now have the exact copy of production database available on your
machine.</p>

<p>This is post 032 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Cloning Windows to a new drive
            </title>
            <link>
                https://garrit.xyz/posts/2022-05-24-cloning-windows-to-a-new-drive?utm_source=rss
            </link>
            <pubDate>
                Tue, 24 May 2022 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>My grandpa has been using his current computer for about 10 years now. After
such a long time, the system has become quite slow and bulky. Back then it was
relatively normal to use a HDD as a primary hard drive, which adds to the slow
experience. It was time for an upgrade!</p>

<blockquote><p><strong>TL;DR</strong>: Use <a href="https://clonezilla.org/">Clonezilla</a> on a live usb stick to
create an exact copy of your old drive onto your new one.</p></blockquote>

<p>I got him a 512 GB SSD, which, conveniently, is the same size of his current
HDD. While installing the new drive alongside his existing one, I thought about
how to copy the existing Windows-installation.</p>

<p>Naïvely, I thought that I could just <code>dd</code> the contents of the HDD onto the new
drive would work, since, <em>every byte is copied as is</em>, or at least that&#39;s what I
thought. Turns out it wasn&#39;t that easy. I&#39;m sure it would&#39;ve worked if I was
more careful, but by default, <code>dd</code> just wipes over each byte, not caring if it
made a mistake. After very long 5 hours, I came back to the PC to see that it
finished copying the 512 GB (yes, it&#39;s not just copying the data, it&#39;s copying
the entire partition!). In a super excited mood, I restarted the PC and selected
the SSD as a boot medium. Aaaaand... nothing. Windows tried to repair some stuff
but it wasn&#39;t successful. I fiddled around with the boot partition a bit, but I
had to give up after an hour or so.</p>

<h2>The second attempt</h2>

<p>After researching a bit (I should&#39;ve done that sooner...) I stumbled across
<a href="https://clonezilla.org/">Clonezilla</a>, a Linux distribution custom-built for
this purpose. I flashed it onto a usb-stick and started the cloning process.
After just 20 minutes (!), it was done cloning the existing data. The process is
extremely simple!</p>

<p>Before rebooting, I disconnected the old drive to make sure that there&#39;s no
funny business going on. Apparently, Windows had to self-adjust UIDs of the
drives, but after a short &quot;Preparing Windows&quot; animation, the system started up
as expected. <strong>Success</strong>!!</p>

<p>The performance of the new hard drive is amazing, at least compared to the HDD
my grandpa had before. Plus, we can use the existing HDD to take full system
backups every now and then, using the same process.</p>

<p>This is post 031 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Fixing Traefik Proxy Issues
            </title>
            <link>
                https://garrit.xyz/posts/2022-03-18-fix-traefik-proxy-issues?utm_source=rss
            </link>
            <pubDate>
                Fri, 18 Mar 2022 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>After changing my proxy from NGINX to Traefik, I noticed that some of my
services started misbehaving.</p>

<p>In particular, my instance of
<a href="https://github.com/NicolasConstant/BirdsiteLive">BirdsiteLive</a>
(<a href="https://birdsite.slashdev.space">birdsite.slashdev.space</a>) had issues
forwarding tweets to the
<a href="https://garrit.xyz/posts/2021-01-18-reasons-the-fediverse-is-better">Fediverse</a>.</p>

<p>The only difference between my old NGINX and my Traefik config were the headers.
I didn&#39;t think that that&#39;s what&#39;s causing the issue, but after digging around a
bit I figured out what&#39;s wrong. I still can&#39;t wrap my head around it entirely,
but it has something to do with forwarding external <code>https</code> requests to internal
<code>http</code> services, since the <code>x-forwarded-</code> headers where missing in the forwarded
requests.</p>

<p>In the world of NGINX, we can instruct the proxy to forward <em>all</em> headers using
this directive:</p>

<p><code>conf
proxy_pass_request_headers      on;
</code></p>

<p>which takes care of the issue. In Traefik, it&#39;s a bit more convoluted. Traefik
can use a combination of &quot;Entrypoints&quot; and middleware to route traffic around.
In my setup, I use a <code>webSecure</code> entrypoint listening for SSL/TLS traffic, and a
<code>web</code> entrypoint that just redirects to <code>webSecure</code>:</p>

<p><code></code>`yaml
entryPoints:
  web:
    address: :80
    http:
      redirections:
        entryPoint:
          to: &quot;websecure&quot;
          scheme: &quot;https&quot;</p>

<p>  websecure:
    address: :443
<code></code>`</p>

<p>Apparently, some services send requests to the <code>web</code> entrypoint, and the
<code>x-forwarded-for</code> headers are dropped. To prevent this, you can set the
<code>proxyProtocol</code> and <code>forwardedHeaders</code> in the <code>web</code> entrypoint to <code>insecure</code>,
like so:</p>

<p><code></code>`yaml
entryPoints:
  web:
    address: :80
    proxyProtocol:
      insecure: true
    forwardedHeaders:
      insecure: true
    # ...</p>

<h1>...</h1>

<p><code></code>`</p>

<p>I&#39;m sure there&#39;s a reason why this is marked as <code>insecure</code>, but it behaves just
like the NGINX counterpart, so I didn&#39;t bother digging deeper into the matter.
Maybe one day I&#39;ll come back to properly fix this.</p>

<p>If you want to read more, check out
<a href="https://medium.com/@_jonas/traefik-kubernetes-ingress-and-x-forwarded-headers-82194d319b0e">this</a>
article on Medium. It explains the issue in more detail.</p>

<hr/>

<p>This is post 025 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                BTRFS on Alpine Linux
            </title>
            <link>
                https://garrit.xyz/posts/2021-12-31-btrfs-on-alpine?utm_source=rss
            </link>
            <pubDate>
                Fri, 31 Dec 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I&#39;m currently in the midst of migrating some of my infrastructure from the cloud
to &quot;on prem&quot;, aka a local server, aka my old PC. I wanted to try alpine linux as
the host OS to see how it behaves as a lightweight server distro.</p>

<p>So far it stands up quite nicely, it has everything you&#39;d expect from a
linux-based operating system. The only problem I encountered was getting BTRFS
to work out of the box. Here are some things you should know when using BTRFS on
Alpine linux.</p>

<h3>Installing BTRFS</h3>

<p>Installing BTRFS is relatively straight forward. Simply install the package and
tell Alpine to load the module on startup:</p>

<p><code>
apk add btrfs-progs
echo btrfs &gt;&gt; /etc/modules
</code></p>

<p>To load the module right away, you can use the following command:</p>

<p><code>
modprobe btrfs
</code></p>

<h3>Mounting a volume</h3>

<p>If you try mounting a btrfs volume via your fstab, you will get an error. This
is because BTRFS does not know about the drives yet when the filesystems are
mounted. To work around this, you can create an OpenRC service that runs a
<code>btrfs scan</code> to detect the drives. To do so, create a service under
<code>/etc/init.d/btrfs-scan</code> with the following content:</p>

<p><code></code>`sh</p>

<h1>!/sbin/openrc-run</h1>

<p>name=&quot;btrfs-scan&quot;</p>

<p>depend() {
  before localmount
}</p>

<p>start() {
  /sbin/btrfs device scan
}
<code></code>`</p>

<p>Make the service executable and register it:</p>

<p><code>
chmod +x /etc/init.d/btrfs-scan
rc-update add btrfs-scan boot
</code></p>

<p>Now, you should be able to add the volume to your <code>/etc/fstab</code>:</p>

<p><code>
UUID=abcdef-0055-4958-990f-1413ed1186ec  /var/data  btrfs   defaults,nofail,subvol=@  0  0
</code></p>

<p>After a reboot, you should be able to see the drive mounted at <code>/var/data</code>.</p>

<h3>Resources</h3>

<ul><li><a href="https://nparsons.uk/blog/using-btrfs-on-alpine-linux">Nathan Parsons - &quot;Using BTRFS on Alpine Linux&quot;</a></li><li><a href="https://gitlab-test.alpinelinux.org/alpine/aports/-/issues/9539">A bug report about this problem</a></li></ul>

<p>This is post 023 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Notes on pruning chinese elms
            </title>
            <link>
                https://garrit.xyz/posts/2021-12-24-notes-on-pruning-chinese-elms?utm_source=rss
            </link>
            <pubDate>
                Fri, 24 Dec 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I recently bought a chinese elm bonsai. To keep it alive and healthy, I devoted
some time to learning how to properly prune it.</p>

<p>Chinese elms are very robust trees with strong growth. Even in the winter
season, I can prune it on a weekly basis. I just watched <a href="https://www.youtube.com/watch?v=Nsvc2Ll1X2A">this
video</a> which gives some helpful
tips on pruning.</p>

<p>I noticed that the author let the tree grow heavily to develop stronger
branches. Until now, I pruned the branches back to one leaf whenever it had
about 5 leaves. However, a branch should only be cut on its woody parts in order
to develop more shoots, and a branch needs some growth in order to turn to wood.
<strong>Don&#39;t prune the branches too early</strong>!</p>

<p>Secondly, I thought that cutting the branches just behind a leaf would lead to
more growth, but I found that this is not true. The branches should be cut just
a bit above the leaf to prevent branch to die back and kill the leaf. It could
also lead to the shoot coming out straight instead of angled, which is not
desired.</p>

<p>Lastly I learned that branches pointing upwards should most likely be pruned,
since they take a lot of energy that the tree could use elsewhere, which is also
pointed out in <a href="https://www.youtube.com/watch?v=93c985zOwhs">this video</a>.</p>

<p>Here&#39;s a picture of my chinese elm just how I bought it. You can clearly tell
that it was cheaply imported from china. It needs a lot of work, but I&#39;m having
a lot of fun learning about this tree!</p>

<p><img alt="Chinese Elm" src="/assets/chinese_elm.jpeg"/></p>

<p>This is post 022 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Server-Side Caching with Apollo GraphQL
            </title>
            <link>
                https://garrit.xyz/posts/2021-10-04-server-side-caching-with-apollo-graphql?utm_source=rss
            </link>
            <pubDate>
                Mon, 04 Oct 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I recently implemented server-side caching for one of our applications at work.
This guide tries to document that I&#39;ve learned. It assumes that you are using
an apollo server of version 3 or higher.</p>

<h3>What is Server-Side Caching?</h3>

<p>The point of server-side caching is to reduce the load of your database by
“remembering” the results of a query for a certain period. If the exact same
query comes in again, that remembered result will be returned.</p>

<p>Caching should be handled with care. You should never enable caching for your
entire application. Instead, you should identify the bottlenecks and develop a
strategy to overcome them.</p>

<h3>Enabling caching on the server</h3>

<p>The Apollo Team has done a great job
<a href="https://www.apollographql.com/docs/apollo-server/performance/caching/">documenting</a>
the caching behavior of their server. To add caching to your existing
Apollo-Server, you first have to add the <code>responseCachePlugin</code> to your
configuration as shown
<a href="https://www.apollographql.com/docs/apollo-server/performance/caching/#caching-with-responsecacheplugin-advanced">here</a>:</p>

<p><code></code>`js
import responseCachePlugin from &quot;apollo-server-plugin-response-cache&quot;;</p>

<p>const server = new ApolloServer({
	// ...other options...
	plugins: [responseCachePlugin()],
});
<code></code>`</p>

<p>Then, you have to configure a cache backend. By default, Apollo Server will
store the caches in RAM, but I’d recommend <a href="https://www.apollographql.com/docs/apollo-server/data/data-sources/#using-memcachedredis-as-a-cache-storage-backend">using
Redis</a>
(or Memcached, if you like), especially if your application is spread across
multiple instances of the same backend.</p>

<p><code></code>`js
const { BaseRedisCache } = require(&quot;apollo-server-cache-redis&quot;);
const Redis = require(&quot;ioredis&quot;);</p>

<p>const server = new ApolloServer({
	// ...
	cache: new BaseRedisCache({
		plugins: [responseCachePlugin()],
		client: new Redis({
			host: &quot;redis-server&quot;,
		}),
	}),
});
<code></code>`</p>

<blockquote><p>Note that you have to use the ioredis library here. node_redis is deprecated
as of v2.6.0 of apollo-server-cache-redis.</p></blockquote>

<p>If everything went well, your server should now know how to cache responses!
This alone won’t get you very far, since it doesn’t know what to cache.</p>

<h3>Telling Apollo what to cache</h3>

<p>To make a type cachable, you have to declare <strong>cache hints</strong>. These properties
can either be set in the
<a href="https://www.apollographql.com/docs/apollo-server/performance/caching/#in-your-resolvers-dynamic">resolver</a>,
or
<a href="https://www.apollographql.com/docs/apollo-server/performance/caching/#in-your-schema-static">statically</a>
in the schema. To keep it simple, this guide will stick to the static method.
Feel free to experiment with the dynamic approach though!</p>

<p>To enable cache hints, simply add the following directive to your schema (you
only have to do this once):</p>

<p><code></code>`gql
enum CacheControlScope {
	PUBLIC
	PRIVATE
}</p>

<p>directive @cacheControl(
	maxAge: Int
	scope: CacheControlScope
	inheritMaxAge: Boolean
) on FIELD_DEFINITION | OBJECT | INTERFACE | UNION
<code></code>`</p>

<p>Now you can add the <code>@cacheControl</code> directive to every type that should be cached.</p>

<p><code></code>`gql</p>

<h1>This type will be cached for 30 seconds</h1>

<p>type Post @cacheControl(maxAge: 30) {
	id: ID!
	title: String
	author: Author
	comments: [Comment]
}
<code></code>`</p>

<p>For security reasons, these conditions are <a href="https://www.apollographql.com/docs/apollo-server/performance/caching/#why-are-these-the-maxage-defaults">very
strict</a>:</p>

<blockquote><p>Our philosophy behind Apollo Server caching is that a response should only be
considered cacheable if every part of that response opts in to being
cacheable.</p></blockquote>

<p>This means that every type needs to explicitly decide how long it should be
cached. According to this note, the example above actually won’t be cached at
all!</p>

<p>Having to specify the <code>maxAge</code> of every type would be tedious, so the authors
have come up with the <code>inheritMaxAge</code> property, which allows the type to
inherit the settings from its parent. So, in order to make our example
cachable, we have to enable cache control for all its subfields, either by
setting the <code>maxAge</code> explicitly or by inheriting it from the parent:</p>

<p><code></code>`gql
type Post @cacheControl(maxAge: 30) {
	id: ID!
	title: String
	author: Author
	comments: [Comment]
}</p>

<p>type Author @cacheControl(inheritMaxAge: true) {
	id: ID!
	name: String
}</p>

<p>type Comment @cacheControl(inheritMaxAge: true) {
	id: ID!
	body: String
}
<code></code>`</p>

<p>Now, whenever you query a <code>Post</code>, it will be thrown in the cache. If you query
the type again within 30 seconds, the query resolver won’t execute. Instead, it
will be read from the cache. Keep in mind that cache hints can also be set on
<code>query</code> and <code>mutation</code> fields. This can be handy if you want to cache the
entire response of a request.</p>

<h3>Gotcha 1: Multiple Response Variations</h3>

<p>The use-case where this topic first came up required us to have different
responses based on the type of the logged in user. An <code>Admin</code> should see a
different result than a <code>Visitor</code>. If you ignore this fact, it could be that a
visitor could see the cache result of a query previously executed by an admin!</p>

<p>This problem can be counteracted by setting extra information in the cache key
via <code>extraCacheKeyData</code> (see
<a href="https://www.apollographql.com/docs/apollo-server/performance/caching/#configuring-reads-and-writes">this</a>
paragraph):</p>

<p><code>js
plugins: [
    responseCachePlugin({
        extraCacheKeyData: (ctx) =&gt; (
            ctx.context.auth.isAdmin
        ),
    }),
],
</code></p>

<p>This example can create two distinct caches: One for users that are marked as
admins, and one for regular users.</p>

<h3>Gotcha 2: User-specific caches</h3>

<p>Besides caching for a group of users, you can also cache responses <a href="https://www.apollographql.com/docs/apollo-server/performance/caching/#identifying-users-for-private-responses">for every
user
individually</a>.
You may have noticed that you can also set a <code>scope</code> field in the cache control
directive. This will only cache the response if a user is logged in:</p>

<p><code>gql
type Post {
	id: ID!
	title: String
	author: Author @cacheControl(maxAge: 10, scope: PRIVATE)
}
</code></p>

<p>Apollo determines if a user is logged in or not, based on if the <code>sessionId</code>
function has returned a value other than <code>null</code>.</p>

<p><code>js
import responseCachePlugin from &quot;apollo-server-plugin-response-cache&quot;;
const server = new ApolloServer({
	// ...other settings...
	plugins: [
		responseCachePlugin({
			sessionId: (requestContext) =&gt;
				requestContext.request.http.headers.get(&quot;sessionid&quot;) || null,
		}),
	],
});
</code></p>

<p>I’m unsure how effective this pattern is, since every user will receive its key
in the cache. This kind of defeats the purpose of server-side caching, which is
meant to reduce load on the database. If you’re trying to cache fields for
individual users, you might also want to take a look at client-side caching via
<a href="https://github.com/appmotion/apollo-augmented-hooks">apollo-augmented-hooks</a>.</p>

<p>This is post 020 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Finally fixing that annoying Cron gotcha
            </title>
            <link>
                https://garrit.xyz/posts/2021-09-13-fixing-an-annoying-cron-gotcha?utm_source=rss
            </link>
            <pubDate>
                Mon, 13 Sep 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>A while ago I went through my server and reworked my <a href="/posts/2021-02-07-storage-setup">storage
setup</a>. As discribed in that blog post, I set
up daily backups to <a href="https://www.backblaze.com/b2/cloud-storage.html">Backblaze
B2</a> using their amazing CLI
through a cron script. A day went by and I noticed that the
<a href="/posts/2021-05-15-healthchecks-io-with-docker">healthcheck</a> didn&#39;t pass.
Unfortunately I didn&#39;t have time to fix this problem immediately, so instead I
executed the command by hand every couple of days. One could argue that this in
total took way more time than the actual fix, but hey, I was lazy. In the end,
I finally dedicated some time to fix this annoying issue.</p>

<p>It turns out that a command executed by cron doesn&#39;t run through sh or bash,
but in a minimal environment without your usual environment-variables. As a
result, my <code>b2</code> command (and many other commands for that matter) won&#39;t run as
expected, if at all. A quick fix is to run your command through bash or sh
explicitly:</p>

<p><code>sh
sh -c &quot;mycommand&quot;
</code></p>

<p>Alternatively, if you want all your entries to use sh or bash, you can set the
<code>SHELL</code> variable at the very beginning of your crontab:</p>

<p><code></code>`sh
SHELL=/bin/bash</p>

<p>15 1 <em> </em> * some_command
<code></code>`</p>

<p><a href="https://askubuntu.com/a/23438">Here</a> is an answer that goes into more detail
about this. Have a great day!</p>

<p>This is post 019 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Fun with Pen and Paper: Race Cars
            </title>
            <link>
                https://garrit.xyz/posts/2021-08-08-fun-with-pen-and-paper-race-cars?utm_source=rss
            </link>
            <pubDate>
                Sun, 08 Aug 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I recently came across some fun games that can be played using nothing but a
pen and some paper. One of those games was called &quot;Race Cars&quot; and I wondered
how on earth you would play a racing game on paper, and the answer is: simple
math!</p>

<h2>The core idea</h2>

<p>The goal of the game is to cross the finish line of a hand-drawn race track
first without hitting the edge. Race cars have a velocity that can be adjusted
each move. If you&#39;re too greedy, you will crash. If you hit the breaks too
early, another player will win.</p>

<h2>How it&#39;s played</h2>

<p>We start out with a hand-drawn race track. The shape really can be anything you
like. Just be creative!</p>

<p><img alt="A race track on paper" src="/assets/paperracer/0.jpg"/></p>

<p>Next, each player makes a cross on the starting line. This resembles the
players racecar. Right now, none of the cars is moving. They have a velocity of
0 on the x axis and 0 on the y axis.</p>

<p><img alt="Two crosses on the starting line" src="/assets/paperracer/1.jpg"/></p>

<p>Each move, a player can accelerate or decelerate his vehicle by 1 on any axis.
The first move of each player is somewhat obvious. They want to accelerate
straight forward. On our race track, that means accelerate by 1 on the y axis.</p>

<p><img alt="A race car made a move" src="/assets/paperracer/2.jpg"/></p>

<p>Next, we can either keep on accelerating like the red player does, or &quot;turn&quot;
our vehicle left by changing our y velocity from 0 to -1, which gives us a
velocity of -1, 1.</p>

<p><img alt="Paper race cars after two moves" src="/assets/paperracer/3.jpg"/></p>

<p>On a long stretch, the red player wants to overtake blue by keeping his foot on
the paddle. Blue on the other hand plays it safe and hits the breaks.</p>

<p><img alt="Paper race cars on a long stretch" src="/assets/paperracer/4.jpg"/></p>

<p>Soon after, red realizes his mistake. Being so busy trying to turn, he&#39;s unable
to hit the breaks. A crash is inevitable. Blue however continues to take the
turn nice and slow. His humble mind brought him victory!</p>

<p><img alt="The red player right before crashing into a wall" src="/assets/paperracer/5.jpg"/></p>

<h2>It&#39;s all about fun</h2>

<p>The concept of this game is very flexible. You can play it with a friend and
see who&#39;s the fastest paper racer, or by yourself and try to beat your record.
Getting tired of just cruising around? Introduce the concept of &quot;items&quot; that
give you a boost or slow down the opponent (think Mario Kart!). If ordinary
tracks are boring, try adding intersections or obstacles. Can you add a third
dimension? The sky is the limit. Get out there and be creative.</p>

<p>This is post 018 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Docker healthchecks using healthchecks.io
            </title>
            <link>
                https://garrit.xyz/posts/2021-05-15-healthchecks-io-with-docker?utm_source=rss
            </link>
            <pubDate>
                Sat, 15 May 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I&#39;m currently in the midst of improving the monitoring of my infrastructure. I
make heavy use of docker and docker-compose for my hosting, so it&#39;s vital to add
monitoring for most of the containers.</p>

<p>I&#39;m hosting my own instance of <a href="https://healthchecks.io/">healthchecks.io</a>.
Their solution to monitoring involves <strong>you</strong> having to ping <strong>them</strong>, instead
of the other way around. This let&#39;s you add healthchecks to virtually anything
that can ping a http-endpoint.</p>

<p>docker-compose let&#39;s you define healthchecks to your config that, when
completing sucessfully, mark the container as &quot;healthy&quot;. The process of adding
such a healthcheck is simple. First, create a new check in your healthchecks.io
account and set the ping interval to 1 minute, or a value you prefer. Then, add
this snippet to your docker-compose file:</p>

<p><code>yaml
app:
  image: nextcloud
  ports:
    - 127.0.0.1:8080:80
  healthcheck:
    test:
      [
        &quot;CMD&quot;,
        &quot;curl&quot;,
        &quot;-f&quot;,
        &quot;https://app-endpoint.tld&quot;,
        &quot;&amp;&amp;&quot;,
        &quot;curl&quot;,
        &quot;-fsS&quot;,
        &quot;-m&quot;,
        &quot;10&quot;,
        &quot;--retry&quot;,
        &quot;5&quot;,
        &quot;-o&quot;,
        &quot;/dev/null&quot;,
        &quot;https://healthchecks.io/ping/&lt;UUID&gt;&quot;,
      ]
    interval: 60s
    timeout: 10s
    retries: 6
</code></p>

<p>Change the first url to the url of your app. The second URL is the endpoint of
your healthchecks.io instance. You can obtain it from the check you configured
earlier.</p>

<p>This configuration will try to ping your application and, if successful, notify
the healthcheck that the application is healthy. If the app is not reachable or
the container is down, the latter request will not be executed and your service
is marked as &quot;down&quot;.</p>

<p>In addition to the healthchecks of my docker containers, I also added basic
healthchecks to my servers cronfiles and its backup-commands.</p>

<p>Do you have any suggestions regarding this topic? Feel free to reach out to me
via Matrix or email!</p>

<p>This is post 017 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                A pretty good guide to pretty good privacy
            </title>
            <link>
                https://garrit.xyz/posts/2021-04-07-pgp-guide?utm_source=rss
            </link>
            <pubDate>
                Wed, 07 Apr 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>In the past week, I&#39;ve been experimenting with PGP, or GPG in particular. In a nutshell, PGP is an encryption standard with a wide range of use cases. For quite some time, I didn&#39;t see the point of keeping a PGP keypair. It seemed like a burden to securely keep track of the key(s). Once you loose it, you will loose the trust of others. But after doing some research on the topic, I found that it&#39;s not actually that much of a hassle, while giving you many benefits.</p>

<h1>The Why</h1>

<p>The most obvious benefit is encrypting and decrypting messages and files. If you upload your public key, I can encrypt our private conversations. Nobody will be able to read what we&#39;re chatting about. If you fear that cloud providers will read through your documents, you can also go ahead and encrypt all of your data with your keypair.</p>

<p>But PGP is not just about encryption. A keypair also gives you a proof of identity. If I see that a piece of work is signed by you, I can be certain that you and only you have worked on this. By signing the keys of people we trust, we build a &quot;chain of trust&quot;. A key with many signatures generally has a higher reputation than one without any signatures.</p>

<p>Take Git commits for example. All it takes is a <code>git config user.email &quot;elon@spacex.com&quot;</code> and I can publish code under a different identity. But if everyone on the team signed their work, they will quickly see that a commit is missing its signature, because I&#39;m simply not able to sign my work with Elon Musk&#39;s keypair. Only if they see a badge like this, they will know that they can trust it.</p>

<p>Your keypair can also come in handy as a SSH key. Before I knew about PGP, I always had to install one key per machine I was working on. With PGP, you only have a single identity, and therefore you only have to install one key on your servers.</p>

<h1>The How</h1>

<p>Let&#39;s first go over the process of setting up a keypair. For this, we will need the <code>gpg</code> command installed on our system. Usually, this is just a <code>&lt;package manager&gt; install gpg</code> away. Then, we will have to generate a keypair. The quickest way to get one is to use <code>gpg --gen-key</code>, but that will make some quirky assumptions about how you want to use your key.</p>

<p>In PGP, there is this concept of a <strong>keyring</strong>. A keyring has one master key and many subkeys. It is generally a good idea to have one fat master key that never expires and many small subkeys that last about a year or two. The benefit of structuring your keys like this is that you will always have your trusted keychain, and in case something goes south, E.g. your key gets compromised, you can replace that subkey and keep your identity.</p>

<p>With that in mind, let&#39;s create our master key. Run <code>gpg --full-gen-key</code> and follow the instructions. You probably want to use the <code>RSA and RSA (default)</code> option, and a key that is 4096 bits long (remember, this is the fat master key that never expires, so it must be secure). The comment can be left blank, unless you know what you are doing with that field. Enter a strong passphrase! If your private key were to get compromised, this passphrase is your last line of defense. Make it long, hard to crack but still rememberable. If everything went well, your key should be generated. Here&#39;s the full example output:</p>

<p><code></code>`
root@c6acc9eb4fd1:/# gpg --full-gen-key
gpg (GnuPG) 2.2.19; Copyright (C) 2019 Free Software Foundation, Inc.
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.</p>

<p>Please select what kind of key you want:
   (1) RSA and RSA (default)
   (2) DSA and Elgamal
   (3) DSA (sign only)
   (4) RSA (sign only)
  (14) Existing key from card
Your selection? 1
RSA keys may be between 1024 and 4096 bits long.
What keysize do you want? (3072) 4096
Requested keysize is 4096 bits
Please specify how long the key should be valid.
         0 = key does not expire
      &lt;n&gt;  = key expires in n days
      &lt;n&gt;w = key expires in n weeks
      &lt;n&gt;m = key expires in n months
      &lt;n&gt;y = key expires in n years
Key is valid for? (0)
Key does not expire at all
Is this correct? (y/N) y</p>

<p>GnuPG needs to construct a user ID to identify your key.</p>

<p>Real name: Foo
Name must be at least 5 characters long
Real name: Foo Bar
Email address: foo@bar.com
Comment:
You selected this USER-ID:
    &quot;Foo Bar <a href="mailto:foo@bar.com">foo@bar.com</a>&quot;</p>

<p>Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O
We need to generate a lot of random bytes. It is a good idea to perform
some other action (type on the keyboard, move the mouse, utilize the
disks) during the prime generation; this gives the random number
generator a better chance to gain enough entropy.
We need to generate a lot of random bytes. It is a good idea to perform
some other action (type on the keyboard, move the mouse, utilize the
disks) during the prime generation; this gives the random number
generator a better chance to gain enough entropy.
gpg: key C8E4854970B7A1A3 marked as ultimately trusted
gpg: revocation certificate stored as &#39;/root/.gnupg/openpgp-revocs.d/4E83F95221E92EDB933F155AC8E4854970B7A1A3.rev&#39;
public and secret key created and signed.</p>

<p>pub   rsa4096 2021-04-07 [SC]
      4E83F95221E92EDB933F155AC8E4854970B7A1A3
uid                      Foo Bar <a href="mailto:foo@bar.com">foo@bar.com</a>
sub   rsa4096 2021-04-07 [E]
<code></code>`</p>

<p>You could stop here and use this key, but let&#39;s instead create some subkeys under that key, to make our lives a bit easier. Take the fingerprint of the key (that large number in the output) and run <code>gpg --edit-key --expert &lt;your fingerprint&gt;</code>. Run <code>addkey</code> three times to add these three keys:</p>

<h2>Signing key</h2>

<p>This key will be used to sign your work (git commits, tags, etc.).</p>

<p><code>
gpg&gt; addkey
</code></p>

<ol><li>Choose option &quot;RSA (set your own capabilities)&quot;, which is currently number 8.</li><li>Toggle E (Encryption) so the &quot;Current allowed actions&quot; only lists Sign and confirm with Q.</li><li>Choose the keysize 2048 (or whatever you prefer).</li><li>Choose the key expire date 1y (or whatever you prefer).</li><li>Confirm twice, then enter your passphrase.</li></ol>

<h2>Encryption key</h2>

<p>This key will be used to encrypt and decrypt messages.</p>

<p><code>
gpg&gt; addkey
</code></p>

<ol><li>Choose option &quot;RSA (set your own capabilities)&quot;, which is currently number 8.</li><li>Toggle S (Sign) so the &quot;Current allowed actions&quot; only lists Encryption and confirm with Q.</li><li>Choose the keysize 2048 (or whatever you prefer).</li><li>Choose the key expire date 1y (or whatever you prefer).</li><li>Confirm twice, then enter your passphrase.</li></ol>

<h2>Authentication key</h2>

<p>This key will be used for SSH authentication.</p>

<p><code>
gpg&gt; addkey
</code></p>

<ol><li>Choose option &quot;RSA (set your own capabilities)&quot;, which is currently number 8.</li><li>Toggle S (Signing), E (Encryption) and A (Authentication) so the &quot;Current allowed actions&quot; only lists Authenticate and confirm with Q.</li><li>Choose the keysize 2048 (or whatever you prefer).</li><li>Choose the key expire date 1y (or whatever you prefer).</li><li>Confirm twice, then enter your passphrase.</li></ol>

<p>Now you should have one key per use case: signing, encrypting and authentication, each with an expiration date:</p>

<p><code>
sec  rsa4096/C8E4854970B7A1A3
     created: 2021-04-07  expires: never       usage: SC
     trust: ultimate      validity: ultimate
ssb  rsa4096/C5F71423813B40A0
     created: 2021-04-07  expires: never       usage: E
ssb  rsa2048/52D4D1D19533D8A5
     created: 2021-04-07  expires: 2022-04-07  usage: S
ssb  rsa2048/072D841844E3F949
     created: 2021-04-07  expires: 2022-04-07  usage: E
ssb  rsa2048/42E4F6E376DD92F6
     created: 2021-04-07  expires: 2022-04-07  usage: A
[ultimate] (1). Foo Bar &lt;foo@bar.com&gt;
</code></p>

<p>Save your key, and optionally upload it to one of the many keyservers:</p>

<p><code></code>`
gpg&gt; save</p>

<p>$ gpg --keyserver keys.openpgp.org  --send-keys foo@bar.com
<code></code>`</p>

<p><strong>Pro tip</strong>: To set a default keyserver (I use <code>keys.opengpg.org</code>, but there are many others out there!), simply add it in your <code>~/.gnupg/gpg.conf</code> file:</p>

<p><code>
keyserver keys.openpgp.org
</code></p>

<p>People can now import your public key via <code>gpg --keyserver keys.opengpg.org --search-keys foo@bar.com</code>.</p>

<p>We&#39;re done with the setup, let&#39;s put our keys to use!</p>

<h2>Code Signing</h2>

<p>To sign your code, you will have to tell git which key to use. Edit your global git options (<code>~/.gitconfig</code>) and add these fields:</p>

<p><code>
[commit]
	gpgsign = true
[tag]
	gpgsign = true
[user]
    name = Foo Bar
	signingkey = 52D4D1D19533D8A5      # Use the ID of your signing key
	email = foo@bar.com
</code></p>

<p>Now, whenever you add a commit, git will sign it with your key. You will have to let your git hosting provider know that this key is yours. Go to your account settings and look for a tab that says &quot;Manage (GPG) keys&quot;. Where this tab is depends on your choice of service. Next, run <code>gpg --export --armor &lt;your master key id&gt;</code> and copy the resulting key into the input field of your git hosting service.</p>

<p>Whenever you push a commit, its signature will be checked against that of your account. And that&#39;s all the magic!</p>

<p><img alt="A signed commit" src="/assets/signed_commit.png"/></p>

<h2>Encrypting messages</h2>

<p>In order to send an encrypted message to someone, you will need his public key. There are numerous ways to obtain a public key of someone. The simplest way is to ask the person for the raw key. If it&#39;s in a text file, you can import it like so:</p>

<p><code>
cat some_key.txt | gpg --import
</code></p>

<p>Oftentimes, people will store their keys on a keyserver, just like you have probably done it. To import someones key, simply search for it on a keyserver. I&#39;ll use my key here as an example.</p>

<p><code>
gpg --keyserver keys.openpgp.org  --search-keys garrit@slashdev.space
</code></p>

<p>Now, your computer should know about my key. To verify that it&#39;s actually me you have imported, you can check if the output of <code>gpg --fingerprint garrit@slashdev.space</code> matches my actual fingerprint: <code>2218 337E 54AA 1DBE 207B 404D BB54 AF7E B093 9F3D</code>.</p>

<p>Optionally, if you trust that the key is actually associated to me, you can sign it. This let&#39;s other people know that you trust me, which helps build a so called &quot;chain of trust&quot;. A key which has been signed by many people is generally more trustworthy than one that has no signatures.</p>

<p><code>
gpg --sign-key garrit@slashdev.space
</code></p>

<p>Now, let&#39;s encrypt a message that only I will be able to read:</p>

<p><code>
printf &quot;If you can read this, you&#39;ve successfully decrypted this message&quot; | gpg --encrypt --sign --armor -r garrit@slashdev.space
</code></p>

<p>Feel free to send this message to my email-address, I&#39;m happy to chat with you!</p>

<p>Decrypting something is as easy as encrypting something. Say the encrypted message lives in <code>message.txt.asc</code>. If you are the recipient, all you have to do is to run <code>gpg --decrypt message.txt.asc</code>.</p>

<h2>SSH</h2>

<p>Your PGP key can also be used as an SSH key to authenticate on your servers.</p>

<p>First we need to add the following to <code>~/.gnupg/gpg-agent.conf</code> to enable SSH support in gpg-agent.</p>

<p><code>
enable-ssh-support
</code></p>

<p>Next, we&#39;ll need to tell gpg which key to use. We need to get the so called <code>keygrip</code> of your authentication key and add it to the <code>~/.gnupg/sshcontrol</code>. The keygrip can be obtained by running <code>gpg -K --with-keygrip</code>. Just copy the keygrip of the authentication key and paste it into the <code>~/.gnupg/sshcontrol</code> file.</p>

<p>Then, we want the ssh agent to know where to look for the key. Put this in your <code>.bashrc</code> file (or corresponding config):</p>

<p><code>
export GPG_TTY=$(tty)
export SSH_AUTH_SOCK=$(gpgconf --list-dirs agent-ssh-socket)
gpgconf --launch gpg-agent
</code></p>

<p>Then, run <code>ssh-add -l</code> to load the key directly.</p>

<p>To get the public ssh key of your keypair, run this command:</p>

<p><code>
gpg --export-ssh-key foo@bar.com
</code></p>

<p>and add the output to the <code>~/.ssh/authorized_keys</code> file on your server. When signing in, you should be prompted to enter the passphrase of your key and then authenticated.</p>

<h2>Closing thoughts</h2>

<p>I hope by now you see the benefits you gain from having a PGP keypair. Whether you find it useful enough to set one up is of course up to you. It is however a good practice to at least sign your git commits as a proof of identity. There are services like <a href="https://keyoxide.org">Keyoxide</a> that let you keep a &quot;public record&quot; of your key, so that other people can verify your identity more easily. If you set up your key, let me know by sending an encrypted message!</p>

<p>This is post 016 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Git's built-in lifesaver
            </title>
            <link>
                https://garrit.xyz/posts/2021-03-13-git-builtin-lifesaver?utm_source=rss
            </link>
            <pubDate>
                Sat, 13 Mar 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>Everyone was in this situation at some point. You wasted a days worth of work by accidentally deleting a branch. But, all hope is not lost! Git never forgets.</p>

<p>Every action, be it committing changes, deleting or switching branches, is noted down by Git. To see your latest actions, you can simply run <code>git reflog</code> (It&#39;s pronounced <code>ref-log</code> but <code>re-flog</code> sounds just as reasonable):</p>

<p><code>
5704fba HEAD@{45}: commit: docs: update changelog
b471457 HEAD@{46}: commit: chore: refactor binop checks in parse_expression
5f5c5d4 HEAD@{47}: commit: fix: struct imports
76db271 HEAD@{48}: commit: chore: fix clippy warning
ac3e11c HEAD@{49}: commit: fix: circular imports
0cbdc88 HEAD@{50}: am: lexer: handle &#39; or &quot; within the string properly
27699f9 HEAD@{51}: commit: docs: spec: add notation
</code></p>

<p>Commits in Git are just data that is not associated by anything. If you accidentally delete a branch, the commits will stay where they are, and you can reference them directly. To recreate your deleted branch, simply run this command:</p>

<p><code>
git checkout -b &lt;branch&gt; &lt;sha&gt;
</code></p>

<p>And that&#39;s it! Your branch is restored. Remember to commit early and often, or prepare to loose your work!</p>

<p>This is post 015 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Strategies to use a terminal alongside (Neo)Vim
            </title>
            <link>
                https://garrit.xyz/posts/2021-02-24-vim-terminal-strategies?utm_source=rss
            </link>
            <pubDate>
                Tue, 23 Feb 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>One thing that bothered me about vim for a long time, was the lack of a terminal
directly in your editor. If I&#39;m not using Vim, I&#39;m most definetely using VSCode
and its built-in Terminal. After searching the webs for possible solutions, I
came across a couple of strategies to achive this.</p>

<h2>Executing single commands</h2>

<p>If you just want to issue a single command without spawning an entire shell,
you can just use the <code>:!</code> command:</p>

<p><code>
:! printf &quot;Hello Sailor&quot;
</code></p>

<h2>Vims builtin terminal</h2>

<p>I couldn&#39;t believe my eyes when I read this, but Vim ships with a builtin
terminal! Executing <code>:term</code> will spawn it in your current buffer. How you
integrate it in your workflow is up to you. You could use tabs or open a
horizontal buffer and spawn it there. I must say that it is rather clunky to
use, since its literally a Vim buffer that forwards stdin and stdout to the
buffer, but it&#39;s there for you to use.</p>

<h2>Vim x Tmux</h2>

<p>Another great alternative is to set up Tmux with two windows, one for Vim and
one for your terminal, and switch between them. This works great on a minimal
system, but on MacOS for example, it is easier to simply use cmd+1 and cmd+2 to
switch between two tabs of the Terminal application.</p>

<h2>Pausing and resuming Vim</h2>

<p>This one is my personal favorite. The idea comes from
<a href="https://stackoverflow.com/a/1258318/9046809">this</a> stackoverflow answer.</p>

<p>The plan is to pause the Vim process and resume it later. To pause Vim, you
press <code>&lt;ctrl&gt;-z</code>. This sends the process in the background. Then, to resume the
process, simply issue the <code>fg</code> command and Vims process resumes in the
foreground.</p>

<h2>Conclusion</h2>

<p>I&#39;m sure there are many more strategies that could be added to this list. I&#39;d be
interested to hear how your setup works! If you liked these techniques, you
might be interested in
<a href="https://fosstodon.org/web/accounts/211905">@lopeztel</a>s
<a href="https://lopeztel.xyz/2021/02/21/my-neovim-cheatsheet/">cheat sheet</a> for Vim.</p>

<p>This is post 014 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Writing good changelogs
            </title>
            <link>
                https://garrit.xyz/posts/2021-02-20-changelogs?utm_source=rss
            </link>
            <pubDate>
                Sat, 20 Feb 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>Today, I finally added a proper changelog to <a href="https://github.com/garritfra/sabre/blob/master/CHANGELOG.md">my current project</a>. My obvious first step was to search the web for <code>changelog.md</code>, since that&#39;s the naming convention many projects are using for their changelog. I was surprised that I was immediately redirected to &quot;<a href="https://changelog.md">changelog.md</a>&quot;, since it is a valid domain name. This website is a great guide on the essense of a good changelog. This is also where I got most of my findings from. Let me walk you through some of the most important ones:</p>

<h2>Changelogs are a vital part of every serious project</h2>

<p>The whole point of a changelog is to keep track of how the project evolves over time. When working with multiple people, it helps getting everyone on the same page. Keeping a changelog reduces a possible monopoly of information, since all contributers know what is going on. Of course, users also benefit from your changelog. They will know what changes they can expect when they do an update.</p>

<h2>Entries should have a standardized format</h2>

<p>Changelogs are mainly meant to be readable by humans. Here are some important points to watch out for when writing a changelog:</p>

<ul><li>Every version of your software (major, minor and patch) should have one section and one section only</li><li>Recent releases should be added at the top of the changelog (reverse chronological order)</li><li>Each version <em>should</em> display its release date in ISO format (YYYY-MM-DD) next to the version name</li></ul>

<h2>What types of changes need to be included?</h2>

<p>You could just go ahead and throw some changes in a big list and call it a day. To make the changelog more readable though, you should categorize every change by its type. Here&#39;s an example of a set of categories that could be included:</p>

<ul><li><strong>Features</strong>: New features or additions</li><li><strong>Fixes</strong>: Bugfixes</li><li><strong>Security</strong> Important changes regarding holes in your security</li><li><strong>Documentation</strong>: Changes or additions in your documentation should go here</li></ul>

<p>This is just an example that illustrates how <strong>I</strong> decided to note down my changes. <a href="https://changelog.md">changelog.md</a> suggests a slightly different convention, but how you&#39;re handling it doesn&#39;t really matter.</p>

<h2>An example</h2>

<p>Here&#39;s an example of how a changelog could look like. It&#39;s taken from <a href="https://github.com/garritfra/sabre">Sabre</a>, a project I&#39;m currently working on. The full changelog can be found <a href="https://github.com/garritfra/sabre/blob/master/CHANGELOG.md">here</a>.</p>

<p><code></code>`md</p>

<h1>Changelog</h1>

<h2>v0.4.0 (2021-02-20)</h2>

<p>This release introduces the concept of structs, alongside many improvements to the documentation.</p>

<p><strong>Features</strong></p>

<ul><li>Assignment operators (#10)</li><li>Structs (#12)</li></ul>

<p><strong>Fixes</strong></p>

<p>None</p>

<p><strong>Documentation</strong></p>

<ul><li>Fixed some typose and broken links</li><li>Document boolean values</li><li>Added this changelog!</li></ul>

<h2>v0.3.0 (2021-02-12)</h2>

<p>This release adds type inference to Sabre. There are also a lot of improvements in terms of documentation. The docs are now at a state that can be considered &quot;usable&quot;.</p>

<p><strong>Features</strong></p>

<ul><li>Type inference</li><li>The <code>any</code> type</li><li>First attempt of LLVM backend</li></ul>

<p><strong>Fixes</strong></p>

<ul><li>Fixed an error when printing numbers</li></ul>

<p><strong>Documentation</strong></p>

<ul><li>Added documentation for for loops</li><li>Added documentation for while loops</li><li>Documented LLVM backend</li><li>Documented comments</li><li>Updated contributing guidelines
<code></code>`</li></ul>

<h2>Personal recommendations</h2>

<p>When releasing a new version, don&#39;t just add an entry to your changelog. You should use <strong>git tags</strong> whenever working with versions, to mark the exact commit of the released version.</p>

<p>Read up on <strong>semantic versioning</strong>! This is the most common convention when it comes to versioning your software. (<a href="https://www.geeksforgeeks.org/introduction-semantic-versioning/">here</a> is a simple guide, <a href="https://semver.org/">here</a> is the official specification).</p>

<p>I&#39;d also advise you to keep a log of your commits in the description of the tag. Here&#39;s a command that does all of this for you:</p>

<p><code>
git tag -a &lt;new release&gt; -m &quot;$(git shortlog &lt;last release&gt;..HEAD)&quot;
</code></p>

<p>So, if you&#39;re releasing version <code>v0.2.0</code> after <code>v0.1.5</code>, you would run this command to tag your current commit with a good commit history:</p>

<p><code>
git tag -a v0.2.0 -m &quot;$(git shortlog v0.1.5..HEAD)&quot;
</code></p>

<p>This is post 013 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                My storage setup (Feburary 2021)
            </title>
            <link>
                https://garrit.xyz/posts/2021-02-07-storage-setup?utm_source=rss
            </link>
            <pubDate>
                Sun, 07 Feb 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I used to rely on Google Drive and Photos to store my entire data. Now, that <a href="https://blog.google/products/photos/storage-changes/">Google has decided to ditch unlimited photo storage in the near future</a> and Google basically being the devil himself, I decided to step up my game and get my hands dirty on a DIY storage solution.</p>

<h2>The goal</h2>

<p>Before I got started, I thought about the expectations I have towards a system like this. It boils down to these four points (in this order): I want my solution to be <strong>resiliant</strong>, <strong>scalable</strong>, <strong>easy to maintain</strong> and <strong>easy to access</strong>. Looking back, I think I met all of these requirements fairly well. Let me walk you through how I managed to do that.</p>

<h2>Data resiliance</h2>

<p>Keeping data on a single device is obviously a really bad idea. Drives eventually fail, which means that your data will be lost. Heck, even my house could burn down, which means that any number of local copies could burn to ashes. To prevent data loss, I strictly adhere to the <a href="https://www.backblaze.com/blog/the-3-2-1-backup-strategy/">3-2-1 backup strategy</a>. A 3-2-1 strategy means having <strong>at least three total copies of your data, two of which are local but on different mediums (read: devices), and at least one copy off-site</strong>. If a drive fails, I can replace it. If my house burns down, I get two new drives and clone my offsite backup to them.</p>

<p>To get an offsite backup, I set up a spare Raspberry Pi with a single large HDD and instructed it to do daily backups of my entire data. I asked a family member if they would be willing to have a tiny computer plugged in to their router 24/7, and they kindly agreed. A Pi and a HDD are very efficient in terms of power, so there is not a lot to worry about.</p>

<h2>Scalability</h2>

<p>I currently don&#39;t have a huge amount of data. If that were to change (i.e. if I continue to shoot a lot of high-res photos and shove them into my setup), I need a way to simply attach more drives, or ones with more capacity. I looked at different file-systems that allowed to easy extendability while also being resiliant.</p>

<p>An obvious candidate was <strong>ZFS</strong>, but there are a couple of reasons I ditched this idea. First of all, it is really hard to get up and running on Raspberry Pi running Linux, since it&#39;s not natively supported by all distributions. This increases the complexity of the setup. Another reason is that I don&#39;t like the way it scales. Please correct me if I&#39;m wrong here, since I only did limited research on this. From what I know though, ZFS can only be extended by shoving a large amount of drives in the setup to achieve perfect redundancy.</p>

<p>In the end, I settled on <strong>BTRFS</strong>. For me, it scratches all the itches that ZFS has. It is baked into the linux kernel, which makes it really easy to install on most distributions, and I can scale it to any number of drives I want. If I find a spare drive somewhere with any storage capacity, I can plug it into the system and it will just work, without having to think about balancing or redundancy shenanigans.</p>

<h2>Maintainability</h2>

<p>I need my setup to be easy to maintain. If a drive fails, I want to be able to replace it within a matter of minutes, not hours. If my host (a Raspberry Pi) bursts into flames, I want to be able to swap in a new one and still access my data. If I&#39;m out and about and something goes south, I want to be able to fix it remotely. BTRFS helps a lot here. It&#39;s really the foundation for all the key points mentioned here. It gives me a simple interface to maintain the data on the drives, and tries to fix issues itself whenever possible.</p>

<p>Exposing random ports to the general public is a huge security risk. To still be able to access the Pi remotely, I set up <strong>an encrypted WireGuard tunnel</strong>. This way, I only have to expose a single port for WireGuard to talk to the device as if I&#39;m sitting next to it.</p>

<h2>Accessibility</h2>

<p>Since the data needs to be accessed frequently, I need a simple interface for it that can be used on any device. I decided to host a <strong>Nextcloud</strong> instance and mount the drive as external storage. Why external storage? Because Nextcloud does some weird thing with the data it stores. If I decide to ditch Nextcloud at some point, I have the data on the disks &quot;as is&quot;, without some sort of abstraction on top of it. This also has the benefit of allowing access from multiple sources. I don&#39;t have to use Nextcloud, but instead can mount the volume as a FTP, SMB or NFS share and do whatever I want with it. From the nextcloud perspective, this has some drawbacks like inefficient caching or file detection, but I&#39;m willing to make that tradeoff.</p>

<h2>In a nutshell</h2>

<p>This entire setup cost me about 150€ in total. Some components were scraped from old PC parts. So, what does the solution look like? Here is the gist:</p>

<ul><li>A Raspberry Pi 4 as a main host and an older Raspberry Pi 3 for offsite backup, both running Raspberry Pi OS</li><li>Two external harddrives in a RAID 1 (mirrored) configuration, running on an external USB3 hub</li><li>A single internal HDD that served no purpose in my old PC, now serving as backup storage</li><li>All drives are using BTRFS</li><li>WireGuard tunnels between main and remote host, as well as most access devices</li><li>Nextcloud on main host, accessible over TLS (if I need to access data from outside the secure tunnel-network)</li><li>SMB share accessible from within the tunnel-network</li><li>Circa 4.5 terabyte total disk size; 1.5 terabyte of usable storage</li><li>Snapper for local incremental backups on main host; BTRBK for remote incremental backups</li><li>Cron jobs for regular backups and repairs (scrub/rebalance)</li></ul>

<p>This is post 010 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                From sudo to doas
            </title>
            <link>
                https://garrit.xyz/posts/2021-01-29-sudo-to-doas?utm_source=rss
            </link>
            <pubDate>
                Fri, 29 Jan 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>You might have heard that there is currently <a href="https://www.qualys.com/2021/01/26/cve-2021-3156/baron-samedit-heap-based-overflow-sudo.txt">a pretty significant vulnerability</a> affecting <code>sudo</code>, the program we all know and love. It is the de facto standard for when you want to run a command as a priviledged user, but that&#39;s really it. Under the hood, sudo is a very powerful tool with a lot of features. It can be used to build out complex permission-systems that span across entire clusters of servers. But all of these features come at a price: <strong>complexity</strong>. Last time I checked, the <a href="https://www.sudo.ws/repos/sudo">source code</a> of sudo had about 330k lines of code (using cloc as a benchmark). This massive complexity plays a large role in its security.</p>

<p>Luckily, there is a <strong>far</strong> more lightweight alternative to sudo called <a href="https://github.com/Duncaen/OpenDoas.git">doas</a>. It essentially does all the things you&#39;d expect from sudo for your average end user. Doas is written in just over 3k lines of code, which, if you think of it, should be more than enough to provide a tool that executes a command as a priviledged user.</p>

<h2>Setup</h2>

<p>While there are packages for <a href="https://github.com/slicer69/doas#installation-via-packagesrepositories">some distibutions</a>, I personally had trouble setting it up on arch using yay (for permission reasons, ironically). I recommend going the extra mile and building it from source, which consists of a few commands and some seconds of your time:</p>

<p><code>sh
git clone https://github.com/slicer69/doas
cd doas
make
sudo make install
</code></p>

<p>Next, you will need to create a config file at <code>/usr/local/etc/doas.conf</code>. Paste the following line into it to give your user root access:</p>

<p><code>sh
permit alice as root
</code></p>

<p>You obviously want to substitute alice with your username. If you have multiple users on your system, simply duplicate that line and substitute the username accordingly. Just restart your terminal window, and you should be able to run programs as root using doas instead of sudo:</p>

<p><code>sh
➜  ~ doas id
uid=0(root) gid=0(root) groups=0(root)
</code></p>

<h2>Bonus: Save your muscle memory</h2>

<p>If you still want to &quot;use&quot; sudo on your machine, you can set up a simple alias in your <code>.{bash|zsh|fish}rc</code>. This will also help with compatibility issues of some scripts, if you decide to ditch the actual sudo from your Box entirely. Just paste this line into your corresponding rc file:</p>

<p><code>
alias sudo=&quot;doas&quot;
</code></p>

<h2>Bonus Bonus: Passwordless authentification</h2>

<p>You can setup doas to skip the password prompt every time you run a command with it. Simply add the <code>nopass</code> option in your doas configuration file:</p>

<p><code>sh
permit nopass alice as root
</code></p>

<p>I hope you found this useful!</p>

<p>This is post 008 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Using Macros in Vim
            </title>
            <link>
                https://garrit.xyz/posts/2021-01-26-vim-macros?utm_source=rss
            </link>
            <pubDate>
                Tue, 26 Jan 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>For a long time, macros in Vim were a huge mystery for me. I knew they existed, but I didn&#39;t know how or why you&#39;d use them. A recent task of mine involved replacing the unsafe operator (<code>!!</code>) in a large kotlin codebase with a null-safe operator (<code>?</code>). This game me a good opportunity to learn about macros. This is a snippet I encountered numerous times:</p>

<p><code>kt
mLeftButton!!.text = &quot;Left&quot;
mLeftButton!!.setOnClickListener(leftListener)
mLeftButton!!.visibility = View.VISIBLE
mRightButton!!.text = &quot;Right&quot;
mRightButton!!.setOnClickListener(rightListener)
mRightButton!!.visibility = View.VISIBLE
</code></p>

<p>You could go ahead and change each line individually, or use the IDEs built in &quot;multi-cursor&quot; tool to save you some work. But, let me show you how I automated this using a Vim-Plugin for Android Studio. Not that the plugin matter, it will work in every Vim-like editor.</p>

<p>A macro in Vim works like this:</p>

<ol><li>Record any sequence of keystrokes and assign them to a key</li><li>Execute that sequence as often as you wish</li></ol>

<p>So let&#39;s see how we&#39;d do that.</p>

<h2>Recording a macro</h2>

<p>To record a macro in Vim, you press <code>q</code> (In normal mode) followed by a key you want to assign the macro to. So, if you wanted to record a macro and save it to the <code>q</code> key, you&#39;d press <code>qq</code>. Vim will notify you that a macro is being recorded. Now, you can press the keystrokes that define your actions. When you&#39;re done, press <code>q</code> in normal mode again to quit your macro.</p>

<p>Coming back to my task, I would want to do the following:</p>

<ol><li><code>qq</code> Record a macro and save it to the <code>q</code> key</li><li><code>_</code> - Jump to the beginning of the line</li><li><code>f!</code> - Find next occurrence of <code>!</code></li><li><code>cw</code> - Change word (Delete word and enter insert mode)</li><li><code>?.</code> - Insert the new characters</li><li><code>&lt;esc&gt;</code> - Enter normal mode</li><li><code>j</code> - go down a line</li><li><code>q</code> - Finish macro</li></ol>

<p>If everything went right, this line:</p>

<p><code>
mLeftButton!!.text = &quot;Left&quot;
</code></p>

<p>Should now look like this:</p>

<p><code>
mLeftButton?.text = &quot;Left&quot;
</code></p>

<p>and your macro should be saved under the <code>q</code> key.</p>

<h2>Using the macro</h2>

<p>In order to use a macro in vim, you press the <code>@</code> key, followed by the key the macro is saved under. Since our macro is defined as <code>q</code>, we&#39;d press <code>@q</code>, and the macro is executed immediately.</p>

<p>Let&#39;s take this further. You might have noticed that I went down a line before closing the macro. This becomes handy when you want to execute it many times. In our case we have 6 lines we want to refactor. 1 line has already been altered, so we have to execute it 5 more times. As per usual with vim, you can execute an action n times by specifying a number before doing the action. Let&#39;s press <code>5@q</code> to execute the macro 5 times. And voila! Our unsafe code is now null-safe.</p>

<p><code>kt
mLeftButton?.text = &quot;Left&quot;
mLeftButton?.setOnClickListener(leftListener)
mLeftButton?.visibility = View.VISIBLE
mRightButton?.text = &quot;Right&quot;
mRightButton?.setOnClickListener(rightListener)
mRightButton?.visibility = View.VISIBLE
</code></p>

<p>Macros are really satisfying to watch, if you ask me!</p>

<p>This is post 007 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Compiling your own kernel
            </title>
            <link>
                https://garrit.xyz/posts/2021-01-15-compiling-your-own-kernel?utm_source=rss
            </link>
            <pubDate>
                Fri, 15 Jan 2021 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>I&#39;m currently in the midst of fiddling around with the kernel a bit, and I figured I just documented my process a bit. Unfortunately, since I&#39;m using a Mac for day to day work, I have to rely on a virtual machine to run anything Linux-related. VirtualBox doesn&#39;t support the most recent kernels (5.9 is the most recent supported one), so there won&#39;t be any cutting-edge development happening here. I decided to use ubuntu as my guest system, since it&#39;s very easy to set up.</p>

<p>So, the first step is to get the sources. You could simply go ahead and download a specific release from <a href="https://kernel.org/">kernel.org</a>, but since I want to hack on it, I decided to go the git-route. Simply download the sources from <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/">their repo</a> and check out the tag you want to build.</p>

<blockquote><p><strong>Note</strong>: this might take a while. Their repository is huge! If you want to only need the <code>HEAD</code> and want to build on bare-metal (no VirtualBox), you could only clone the latest commit using <code>git clone git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git --depth=1</code>.</p></blockquote>

<p>Next up, you need to generate a <code>.config</code>. This file describes which features you want to compile into your kernel. To make a generic config that only compiles drivers for the hardware of your system, you can run the following commands:</p>

<p><code></code>`bash</p>

<h1>Copy the config of your current kernel into the repo</h1>

<p>make oldconfig</p>

<h1>Only enable modules that are currently used by the system</h1>

<p>make localmodconfig
<code></code>`</p>

<p>Now, let&#39;s get to actually compiling the kernel. In my case, I assigned 4 cores to my VM. The <code>-j</code> option tells make to run 4 jobs in parallel.</p>

<blockquote><p><strong>Caution</strong>: Just providing -j will freeze your system, since make will try to launch an infinite amount of processes!</p></blockquote>

<p><code>
make -j4
</code></p>

<p>Again, this might take some time. Go for a walk, get a coffee or watch your favorite TV-show. After compilation has finished, we need to install the kernel. To do so, run the following commands:</p>

<p><code>
sudo make modules_install
sudo make install
</code></p>

<p>In order to boot, we need to tell our bootloader about our new kernel. Run this command to update your grub config:</p>

<p><code>
sudo update-grub2
</code></p>

<p>And voila! Your new kernel should be ready.</p>

<p>Reboot the system, and grub should pick up the new kernel and boot to it. If that&#39;s not the case, you should be able to pick the kernel from the grub menu under <code>advanced options</code>.</p>

<h2>Retrospective</h2>

<p>I found that building my own kernel is a highly educational and fun experience. Using VirtualBox is a pain in the <code>/dev/null</code> to work with, since it has to add a lot of overhead to the system in order to work. You sometimes have to wait over 6 month until the support for a new kernel arrives. This problem should not apply if you compile on bare metal systems.</p>

<p>Thanks for your time!</p>

<p>This is post 004 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Booleans are wasted memory
            </title>
            <link>
                https://garrit.xyz/posts/2020-11-17-booleans-are-wasted-memory?utm_source=rss
            </link>
            <pubDate>
                Tue, 17 Nov 2020 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>A boolean is either <code>true</code> or <code>false</code>. That translates to <code>1</code> or <code>0</code>. If you think that one bit is enough to store this information, you&#39;d be wrong.</p>

<p>In order to keep the binary layout of a program simple and convenient, most languages store information in 8 bit (1 byte) blocks.
If you allocate a <code>bool</code> in Rust or (most) other languages that are based on LLVM, <a href="https://llvm.org/docs/LangRef.html#simple-constants">it will take up 1 <code>i1</code>, or 1 byte of memory</a>. If you allocate a boolean value in C, you will get <a href="https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/stdbool.h.html">an integer constant with a value of either 1 or 0</a>.</p>

<p>If you find yourself having to store multiple boolean states somewhere, you might simply declare those booleans and call it a day:</p>

<p><code></code>`c</p>

<h1>include &lt;stdbool.h&gt;</h1>

<h1>include &lt;stdio.h&gt;</h1>

<p>int main()
{
    bool can<em>read = true;
    bool can</em>write = true;
    bool can_execute = false;</p>

<pre><code>if (can_read)
    printf(&quot;read bit set\n&quot;);
if (can_write)
    printf(&quot;write bit set\n&quot;);
if (can_execute)
    printf(&quot;execute bit set\n&quot;);

// Output:
// read bit set
// write bit set</code></pre>

<p>}
<code></code>`</p>

<h2>We can do better than this</h2>

<p>An alternative approach to store boolean values is to share a &quot;chunk&quot; of bits with other values. This is usually done using bitwise operations:</p>

<p><code></code>`c</p>

<h1>include &lt;stdbool.h&gt;</h1>

<h1>include &lt;stdio.h&gt;</h1>

<p>// Define permissions</p>

<h1>define PERM_NONE       0b000</h1>

<h1>define PERM_READ       0b001</h1>

<h1>define PERM_WRITE      0b010</h1>

<h1>define PERM_EXECUTE    0b100</h1>

<h1>define PERM<em>ALL        PERM</em>READ | PERM<em>WRITE | PERM</em>EXECUTE</h1>

<p>int main()
{
    // Allocate 1 byte for permissions
    char permissions = PERM<em>READ | PERM</em>WRITE;</p>

<pre><code>if (permissions &amp; PERM_READ)
    printf(&quot;write bit set\n&quot;);
if (permissions &amp; PERM_WRITE)
    printf(&quot;read bit set\n&quot;);
if (permissions &amp; PERM_EXECUTE)
    printf(&quot;execute bit set\n&quot;);

// Output:
// read bit set
// write bit set</code></pre>

<p>}
<code></code>`</p>

<p>This example still wastes 5 bits since we only use 3 out of 8 possible bits of the char type, but I&#39;m sure you get the point. Allocating 3 boolean values independently would waste 7 * 3 = 21 bits, so it&#39;s a massive improvement. Whenever you find yourself needing multiple boolean values, think twice if you can use this pattern.</p>

<p>Microcontrollers have a very constrainted environment, therefore bitwise operations are essential in those scenarios. 7 wasted bits are a lot if there are only 4 kb of total memory available. For larger systems we often forget about these constraints, until they add up.</p>

<h2>My Plea</h2>

<ul><li>Be mindful about the software you create.</li><li>Appreciate the resources at your disposal.</li></ul>]]>
            </description>
        </item>
        <item>
            <title>
                The Patch-Based Git Workflow
            </title>
            <link>
                https://garrit.xyz/posts/patch-based-git-workflow?utm_source=rss
            </link>
            <pubDate>
                Mon, 28 Sep 2020 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>If you have ever contributed to an open source project, chances are you have opened a pull request on GitHub or a similar platform to present your code to the maintainers. While this is a very approachable way of getting your code reviewed, some projects have decided against using pull requests and instead accept patches via email.</p>

<h2>An introduction to patches</h2>

<p>A patch is essentially a git commit expressed in plain text. It describes what commit the change is based on, and what has changed. A basic patch looks like this:</p>

<p><code></code>`
From 92132241233033a123c4fa833449d6a0d550219c Mon Sep 17 00:00:00 2001
From: Bob <a href="mailto:bob@example.com">bob@example.com</a>
Date: Tue, 25 May 2009 15:42:16 +0200
Subject: [PATCH 1/2] first change</p>

<hr/>

<p> test.txt |    1 +-
 1 files changed, 1 insertions(+), 1 deletions(-)</p>

<p>diff --git a/test.txt b/test.txt
index 7634da4..270eb95 100644
--- a/test.txt
+++ b/test.txt
@@ -1 +1 @@
-Hallo Bob
+Hallo Alice!
<code></code>`</p>

<p>As you can see, it is very readable for both the reviewer and the machine.</p>

<h2>Sending and receiving patches</h2>

<p>The easiest way you can generate a patch from a commit is to use <code>git-format-patch</code>:</p>

<p><code>
git format-patch HEAD^
</code></p>

<p>This will generate a <code>.patch</code> file, that can be embedded into an email and sent to the maintainers. Oftentimes they will then reply to your mail with some inline comments about your code.</p>

<p>To simplify this process further, git has the <code>send-email</code> command, which let&#39;s you send the patch directly to someone without needing to embed it manually. I won&#39;t go into details about this, but there is a <a href="https://git-send-email.io/">well written guide</a> on how to set it up.</p>

<p>If you have received a patch from someone, you can apply it to your tree with the <code>am</code> (apply mail) command:</p>

<p><code>
git am &lt; 0001-first-change.patch
</code></p>

<p>check your <code>git log</code> to see the patch in form of the latest commit.</p>

<h2>Why even bother</h2>

<p>You might think that this is just a silly and outdated approach to collaborative development. &quot;Why not simply open a pull request?&quot; you might ask. Some projects, especially low-level oriented ones like the Linux kernel, do not want to rely on third-party platforms like GitHub to host their code, with good reasons:</p>

<ol><li>Everyone can participate! You don&#39;t need to register an account on some proprietary website to collaborate in a project that uses a patch-based workflow. You don&#39;t even have to expose your identity, if you don&#39;t want to. All you need is an email-address, and frankly most of us have one.</li><li>It&#39;s plain simple! Once you get used to generating and applying patches on the command line, it is in fact easier and faster than opening a pull request in some clunky GUI. It doesn&#39;t get simpler than plain text.</li><li>It is rewarding! Once you have submitted a patch to a project, there is no better feeling than getting a simple &quot;Applied, thanks!&quot; response from a maintainer. And if it&#39;s a response that contains feedback rather than an approval, it feels even better to submit that reworked code again and get it eventually applied.</li></ol>

<h2>Conclusion</h2>

<p>The patch-based workflow is an alternative way to collaborate with developers. If it helps you in your day to day business depends on the projects you are contributing to, but in the end it is always good to have many tools under your belt.</p>]]>
            </description>
        </item>
        <item>
            <title>
                Quick Tip! Setting up a lightweight Server-Client VPN with wireguard
            </title>
            <link>
                https://garrit.xyz/posts/lightweight-vpn-with-wireguard?utm_source=rss
            </link>
            <pubDate>
                Wed, 19 Aug 2020 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>This blog post has been taken over from my <a href="https://garrit.xyz/til">collection of &quot;Today I Learned&quot; articles</a>.</p>

<p>You can easily set up a private network of your devices. This way you can &quot;talk&quot; to your phone, raspberry pi etc. over an <strong>encrypted</strong> network, with simple IP-addresses.</p>

<p><img alt="" src="https://images.unsplash.com/photo-1505659903052-f379347d056f?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=2550&amp;q=80"/></p>

<p>Firstly, install wireguard on all of your systems. Simply install the <code>wireguard</code> package from your package manager respectively. Check out <a href="https://www.wireguard.com/install/">the official installation guide</a> if you can&#39;t find the package. If you&#39;re on debian, try <a href="https://wiki.debian.org/WireGuard?action=show&amp;redirect=Wireguard">this</a> guide. There&#39;s also an app for Android, iOS and MacOS.</p>

<p>Every participent (Client and server) needs a key-pair. To generate this, run this command first on the server, and on all clients:</p>

<p><code>bash
wg genkey | tee wg-private.key | wg pubkey &gt; wg-public.key
</code></p>

<p>It might make sense to do this as root. This way you don&#39;t have to type <code>sudo</code> with every command.</p>

<h2>Server Configuration</h2>

<p>You will need to create a configuration for the server. Save this template at <code>/etc/wireguard/wg0.conf</code>, and replace the fields where needed:</p>

<p><code></code>`conf
[Interface]
PrivateKey = &lt;Server private key from wg-private.key&gt;
Address = 10.0.0.1/24 # IP Address of the server. Using this IP Address, you can assign IPs ranging from 10.0.0.2 - 10.0.0.254 to your clients
ListenPort = 51820 # This is the standard port for wireguard</p>

<h1>The following fields will take care of routing</h1>

<p>PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE
PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE</p>

<h1>Laptop</h1>

<p>[Peer]
PublicKey = &lt;Public Key of Laptop Client&gt;
AllowedIPs = 10.0.0.2/32 # The client will be reachable at this address</p>

<h1>Android Phone</h1>

<p>[Peer]
PublicKey = &lt;Public Key of Phone Client&gt;
AllowedIPs = 10.0.0.3/32</p>

<h1>...</h1>

<p><code></code>`</p>

<p>Then run <code>wg-quick up wg0</code> to start the wireguard interface with the configuration from <code>/etc/wireguard/wg0</code>.</p>

<h2>Setting up clients</h2>

<p>Setting up clients is very similar to the server setup process. Generate a keypair on each client, save the following config to <code>/etc/wireguard/wg0.conf</code> and replace the nessessary fields:</p>

<p><code></code>`conf
[Interface]
PrivateKey = &lt;Client Private Key from wg-private.key&gt;
Address = 10.0.0.2/32 # The fixed address of the client. Needs to be specified in the server config as well</p>

<p>[Peer]
PublicKey = &lt;Server Public key&gt;
AllowedIPs = 10.0.0.0/24 # Routes all traffic in this subnet to the server. If you want to tunnel all traffic through the wireguard connection, use 0.0.0.0/0 here instead
Endpoint = &lt;Public Server IP&gt;:51820
PersistentKeepalive = 25 # Optional. Will ping the server every 25 seconds to remain connected.
<code></code>`</p>

<p>On every client, run <code>wg-quick up wg0</code> to start the interface using the config at <code>/etc/wireguard/wg0.conf</code>.</p>

<p>This whole proccess might be easier on GUIs like Android or MacOS.</p>

<p>Now, try to ping your phone from your laptop:</p>

<p><code>
ping 10.0.0.3
PING 10.0.0.3 (10.0.0.3) 56(84) bytes of data.
64 bytes from 10.0.0.3: icmp_seq=1 ttl=64 time=5382 ms
64 bytes from 10.0.0.3: icmp_seq=2 ttl=64 time=4364 ms
</code></p>

<h3>References</h3>

<ul><li><a href="https://www.wireguard.com/">Official Documentation</a></li><li><a href="https://www.stavros.io/posts/how-to-configure-wireguard/">https://www.stavros.io/posts/how-to-configure-wireguard/</a></li></ul>]]>
            </description>
        </item>
        <item>
            <title>
                Testing isn't hard
            </title>
            <link>
                https://garrit.xyz/posts/testing-isnt-hard?utm_source=rss
            </link>
            <pubDate>
                Fri, 08 Nov 2019 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>&quot;I write two tests before implementing a method&quot;, &quot;My project has 90% coverage&quot;.</p>

<p>I don&#39;t know about you, but that&#39;s something I don&#39;t hear very often. But why is that?</p>

<p>Testing is not even that difficult to do, but yet it is always coming short in my projects. About a year ago, I&#39;ve tried to implement tests in my React applications with little success, mostly because integrating <code>enzyme</code> and configuring it correctly is not that intuitive as a relatively new developer. I want to share my (partly opinionated) approach to JavaScript testing with <code>jest</code>, to get you started. In a later post I will demonstrate a way to implement <code>enzyme</code> into your React projects.</p>

<h1>The basics of testing JavaScript functions</h1>

<p>To get started, you need a npm-project. I don&#39;t think I have to explain that, but just in case:</p>

<p><code>bash
mkdir awesome-testing-project
cd awesome-testing-project
npm init -y
</code></p>

<p>Of course, we need a unit we want to test. What about a method that returns the first element of an array?</p>

<p><code>js
module.exports = function firstElement(arr) {
	return arr[1];
};
</code></p>

<p>You already spotted a bug, huh? Let&#39;s keep it simple for now.</p>

<p>Install and initialize Jest, an open-source testing framework maintained by Facebook. When initializing, you should check every question with <code>y</code>.</p>

<p><code>bash
npm i --save-dev jest
npx jest --init
</code></p>

<p>Next up, we need to define our first test. Conventionally, we create a folder named <code>__tests__</code> in the directory of the module we want to test. inside it, there should be a file named <code>&lt;module&gt;.test.js</code>. Something like this:</p>

<p><code>bash
▶ tree
.
├── package.json
└── src
    ├── __tests__
    │   └── firstElement.test.js
    └── firstElement.js
</code></p>

<p>Jest provides global functions that do not need to be imported in a file. A simple test can look like this:</p>

<p><code></code>`js
const firstElement = require(&quot;../firstElement.js&quot;);</p>

<p>test(&quot;firstElement gets first element of array&quot;, () =&gt; {
	expect(firstElement([1, 2])).toBe(1);
});
<code></code>`</p>

<p><code>expect</code> is another word for <code>assert</code>. If you ask me, &quot;Expect firstElement of [1, 2] to be 1&quot; sounds reasonably english, doesn&#39;t it? After defining the test, all there is to do left is to run the <code>npm test</code> command, which has been created for us by running <code>npx jest --init</code> earlier.</p>

<p><code></code>`bash
▶ npm test</p>

<blockquote><p>jest</p></blockquote>

<p> FAIL  src/<strong>tests</strong>/firstElement.test.js
  ✕ firstElement (6ms)</p>

<p>  ● firstElement</p>

<pre><code>expect(received).toBe(expected) // Object.is equality

Expected: 1
Received: 2

  2 |
  3 | test(&#39;firstElement&#39;, () =&gt; {
&gt; 4 |   expect(firstElement([1, 2])).toBe(1);
    |                                ^
  5 | });
  6 |

  at Object.&lt;anonymous&gt;.test (src/__tests__/firstElement.test.js:4:32)</code></pre>

<p>Test Suites: 1 failed, 1 total
Tests:       1 failed, 1 total
Snapshots:   0 total
Time:        1.1s
Ran all test suites.
npm ERR! Test failed.  See above for more details.
<code></code>`</p>

<p>Whoops! Looks like we have found a bug! Let&#39;s fix it by adjusting the index of the return value in the firstElement function:</p>

<p><code>js
module.exports = function firstElement(arr) {
	return arr[0];
};
</code></p>

<p>And after rerunning <code>npm test</code>:</p>

<p><code></code>`bash
▶ npm test</p>

<blockquote><p>jest</p></blockquote>

<p> PASS  src/<strong>tests</strong>/firstElement.test.js
  ✓ firstElement (4ms)</p>

<p>Test Suites: 1 passed, 1 total
Tests:       1 passed, 1 total
Snapshots:   0 total
Time:        0.666s, estimated 2s
Ran all test suites.
<code></code>`</p>

<p>Yay, your first unit test! Of course, there is much more to find out about the Jest framework. To see a full guide, read the <a href="https://jestjs.io/">official docs</a>.</p>

<p>I have prepared a <a href="https://github.com/garritfra/react-parcel-boilerplate">template repository</a> for building react apps. It also uses Jest to run tests, you don&#39;t have to worry about a thing! If you found this interesting, consider checking out my other blog posts, and/or check out my <a href="https://github.com/garritfra">GitHub</a>!</p>]]>
            </description>
        </item>
        <item>
            <title>
                Fighting Array Functions with ES6
            </title>
            <link>
                https://garrit.xyz/posts/fighting-array-functions-with-es6?utm_source=rss
            </link>
            <pubDate>
                Sun, 07 Apr 2019 00:00:00 +0000
            </pubDate>
            <description>
                <![CDATA[<p>Yesterday, I came across an interesting bug regarding JavaScript Arrays, and I wanted to share my approach to fixing it.
At a basic level, I wanted to pass part of an array to a function, but wanted to use the original array later on.</p>

<p><code>js
let arr = [1, 2, 3, 4, 5]
let something = arr.splice(0, 3)
do(something) // []
DoSomethingWithOriginal(arr)
</code></p>

<p>Thinking that Array.prototype functions don’t mutate the array directly, I moved on with my day. This lead to a bunch of problems down the line.
Some array methods in the EcmaScript specification are designed to mutate arrays, while others do not.</p>

<h3>Non-mutating functions</h3>

<ul><li>Array.prototype.map()</li><li>Array.prototype.slice()</li><li>Array.prototype.join()</li><li>…</li></ul>

<p>These functions do not mutate the array they are called on. For example:</p>

<p><code>js
let arr = [1, 2, 3, 4, 5];
let partOfArr = arr.slice(1, 2);
console.log(partOfArr); // [2, 3]
console.log(arr); // [1, 2, 3, 4, 5]
</code></p>

<h3>Mutating functions</h3>

<ul><li>Array.prototype.sort()</li><li>Array.prototype.splice()</li><li>Array.prototype.reverse()</li><li>…</li></ul>

<p>These methods mutate the array directly. This can lead to unreadable code, as the value can be manipulated from anywhere. For example:</p>

<p><code>js
let arr = [5, 2, 4];
arr.sort();
console.log(arr); // [2, 4, 5]
</code></p>

<p>To me, it is very unclear, which functions do, and which don’t mutate arrays directly. But, there’s a simple trick you can use to stop letting the functions mutate arrays directly, ultimately leading to more readable and reliable code.</p>

<h2>Enter: The ES6 Spread Operator!</h2>

<p><img alt="Spread Operator" src="https://images.unsplash.com/photo-1518297056586-889f796873e0?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1225&amp;q=80"/></p>

<p>Take a look at this snippet:</p>

<p><code>js
let arr = [3, 5, 1, 2, 4];
let sorted = [...arr].sort();
console.log(arr); // [3, 5, 1, 2, 4]
console.log(sorted); // [1, 2, 3, 4, 5]
</code></p>

<p>Voilà! We have a sorted array, and the original one is also around. The spread operator(<code>[...arr]</code>) is used to create a new array with every value of arr .
You can use this for arrays, as well as objects:</p>

<p><code>js
let obj = {
	field: &quot;example&quot;,
};
let extendedObj = {
	...obj,
	anotherField: 42,
};
console.log(extendedObj.field); // &quot;example&quot;
</code></p>

<h2>Conclusion</h2>

<p>ES6 brought us awesome features like let and const assignments, as well as arrow functions. A more unknown feature however is the spread operator. I hope you now know how to use the spread operator, and that you can adopt it for cleaner and simpler code.</p>]]>
            </description>
        </item>
    </channel>
</rss>