<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width, initial-scale=1"/><meta charSet="utf-8"/><title>My storage setup (Feburary 2021) | Garrit&#x27;s Notes</title><meta name="Description" content="Generalist developer writing about fullstack development, system administration and free software."/><link rel="icon" type="image/svg+xml" href="/favicon.svg"/><link rel="webmention" href="https://webmention.io/garrit.xyz/webmention"/><link rel="pingback" href="https://webmention.io/garrit.xyz/xmlrpc"/><link href="https://cdn.jsdelivr.net/npm/shareon@2/dist/shareon.min.css" rel="stylesheet"/><meta name="next-head-count" content="8"/><link rel="preload" href="/_next/static/css/d7b5a815c8fda5f6.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d7b5a815c8fda5f6.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-ee7e63bc15b31913.js" defer=""></script><script src="/_next/static/chunks/framework-305cb810cde7afac.js" defer=""></script><script src="/_next/static/chunks/main-8f5aa507902b2d74.js" defer=""></script><script src="/_next/static/chunks/pages/_app-d8a9932a00f6e475.js" defer=""></script><script src="/_next/static/chunks/996-d63ad7fae077247e.js" defer=""></script><script src="/_next/static/chunks/146-00194b356792ddd7.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bpost%5D-5074f726cf5b0735.js" defer=""></script><script src="/_next/static/VYPhm2fTttAtwRaZriPSY/_buildManifest.js" defer=""></script><script src="/_next/static/VYPhm2fTttAtwRaZriPSY/_ssgManifest.js" defer=""></script></head><body><div id="__next"><section class="layout"><header class="header"><nav class="nav" role="navigation" aria-label="main navigation"><div class="header__container"><a href="/" class="header__container__logo underlined">Garrit&#x27;s Notes</a></div><ul class="header__links"><li><a href="/posts" class="underlined">Blog</a></li><li><a href="/contact" class="underlined">Contact</a></li><li><a href="/cv" class="underlined">Resume</a></li></ul></nav></header><div class="content"><article class="page h-entry"><div class="page__info"><h1 class="p-name">My storage setup (Feburary 2021)</h1><time class="page__info__date">Feb 07 2021</time></div><div class="page__body e-content"><p>I used to rely on Google Drive and Photos to store my entire data. Now, that <a href="https://blog.google/products/photos/storage-changes/">Google has decided to ditch unlimited photo storage in the near future</a> and Google basically being the devil himself, I decided to step up my game and get my hands dirty on a DIY storage solution.</p>
<h2 id="the-goal" level="2">The goal</h2>
<p>Before I got started, I thought about the expectations I have towards a system like this. It boils down to these four points (in this order): I want my solution to be <strong>resiliant</strong>, <strong>scalable</strong>, <strong>easy to maintain</strong> and <strong>easy to access</strong>. Looking back, I think I met all of these requirements fairly well. Let me walk you through how I managed to do that.</p>
<h2 id="data-resiliance" level="2">Data resiliance</h2>
<p>Keeping data on a single device is obviously a really bad idea. Drives eventually fail, which means that your data will be lost. Heck, even my house could burn down, which means that any number of local copies could burn to ashes. To prevent data loss, I strictly adhere to the <a href="https://www.backblaze.com/blog/the-3-2-1-backup-strategy/">3-2-1 backup strategy</a>. A 3-2-1 strategy means having <strong>at least three total copies of your data, two of which are local but on different mediums (read: devices), and at least one copy off-site</strong>. If a drive fails, I can replace it. If my house burns down, I get two new drives and clone my offsite backup to them.</p>
<p>To get an offsite backup, I set up a spare Raspberry Pi with a single large HDD and instructed it to do daily backups of my entire data. I asked a family member if they would be willing to have a tiny computer plugged in to their router 24/7, and they kindly agreed. A Pi and a HDD are very efficient in terms of power, so there is not a lot to worry about.</p>
<h2 id="scalability" level="2">Scalability</h2>
<p>I currently don&#x27;t have a huge amount of data. If that were to change (i.e. if I continue to shoot a lot of high-res photos and shove them into my setup), I need a way to simply attach more drives, or ones with more capacity. I looked at different file-systems that allowed to easy extendability while also being resiliant.</p>
<p>An obvious candidate was <strong>ZFS</strong>, but there are a couple of reasons I ditched this idea. First of all, it is really hard to get up and running on Raspberry Pi running Linux, since it&#x27;s not natively supported by all distributions. This increases the complexity of the setup. Another reason is that I don&#x27;t like the way it scales. Please correct me if I&#x27;m wrong here, since I only did limited research on this. From what I know though, ZFS can only be extended by shoving a large amount of drives in the setup to achieve perfect redundancy.</p>
<p>In the end, I settled on <strong>BTRFS</strong>. For me, it scratches all the itches that ZFS has. It is baked into the linux kernel, which makes it really easy to install on most distributions, and I can scale it to any number of drives I want. If I find a spare drive somewhere with any storage capacity, I can plug it into the system and it will just work, without having to think about balancing or redundancy shenanigans.</p>
<h2 id="maintainability" level="2">Maintainability</h2>
<p>I need my setup to be easy to maintain. If a drive fails, I want to be able to replace it within a matter of minutes, not hours. If my host (a Raspberry Pi) bursts into flames, I want to be able to swap in a new one and still access my data. If I&#x27;m out and about and something goes south, I want to be able to fix it remotely. BTRFS helps a lot here. It&#x27;s really the foundation for all the key points mentioned here. It gives me a simple interface to maintain the data on the drives, and tries to fix issues itself whenever possible.</p>
<p>Exposing random ports to the general public is a huge security risk. To still be able to access the Pi remotely, I set up <strong>an encrypted WireGuard tunnel</strong>. This way, I only have to expose a single port for WireGuard to talk to the device as if I&#x27;m sitting next to it.</p>
<h2 id="accessibility" level="2">Accessibility</h2>
<p>Since the data needs to be accessed frequently, I need a simple interface for it that can be used on any device. I decided to host a <strong>Nextcloud</strong> instance and mount the drive as external storage. Why external storage? Because Nextcloud does some weird thing with the data it stores. If I decide to ditch Nextcloud at some point, I have the data on the disks &quot;as is&quot;, without some sort of abstraction on top of it. This also has the benefit of allowing access from multiple sources. I don&#x27;t have to use Nextcloud, but instead can mount the volume as a FTP, SMB or NFS share and do whatever I want with it. From the nextcloud perspective, this has some drawbacks like inefficient caching or file detection, but I&#x27;m willing to make that tradeoff.</p>
<h2 id="in-a-nutshell" level="2">In a nutshell</h2>
<p>This entire setup cost me about 150â‚¬ in total. Some components were scraped from old PC parts. So, what does the solution look like? Here is the gist:</p>
<ul>
<li>A Raspberry Pi 4 as a main host and an older Raspberry Pi 3 for offsite backup, both running Raspberry Pi OS</li>
<li>Two external harddrives in a RAID 1 (mirrored) configuration, running on an external USB3 hub</li>
<li>A single internal HDD that served no purpose in my old PC, now serving as backup storage</li>
<li>All drives are using BTRFS</li>
<li>WireGuard tunnels between main and remote host, as well as most access devices</li>
<li>Nextcloud on main host, accessible over TLS (if I need to access data from outside the secure tunnel-network)</li>
<li>SMB share accessible from within the tunnel-network</li>
<li>Circa 4.5 terabyte total disk size; 1.5 terabyte of usable storage</li>
<li>Snapper for local incremental backups on main host; BTRBK for remote incremental backups</li>
<li>Cron jobs for regular backups and repairs (scrub/rebalance)</li>
</ul>
<p>This is post 010 of <a href="https://100daystooffload.com/">#100DaysToOffload</a>.</p><hr/><p><a href="mailto:garrit@slashdev.space?subject=Re: My%20storage%20setup%20(Feburary%202021)">Reply via E-Mail</a></p><a href="https://www.buymeacoffee.com/garrit" target="_blank"><img src="https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&amp;emoji=&amp;slug=garrit&amp;button_colour=FFDD00&amp;font_colour=000000&amp;font_family=Cookie&amp;outline_colour=000000&amp;coffee_colour=ffffff"/></a><p class="page__tag-list"><svg class="page__tag-icon" xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><a href="/posts?tags=infrastructure">#<!-- -->infrastructure</a><a href="/posts?tags=guide">#<!-- -->guide</a><a href="/posts?tags=100DaysToOffload">#<!-- -->100DaysToOffload</a><a href="/posts?tags=homelab">#<!-- -->homelab</a></p><div class="shareon"><a class="facebook"></a><a class="linkedin"></a><a class="mastodon"></a><a class="pocket"></a><a class="reddit"></a><a class="telegram"></a><a class="twitter"></a><a class="whatsapp"></a></div><hr/><h2>Continue Reading</h2><div><div class="blog__list__post"><time class="blog__list__post__date">Jun 01 2023</time><br/><a href="/posts/2023-06-01-single-page-applications-on-github-pages">Single Page Applications on GitHub Pages</a></div><div class="blog__list__post"><time class="blog__list__post__date">Apr 28 2023</time><br/><a href="/posts/2023-04-28-serverless-framework-retrospective">Serverless Framework Retrospective</a></div><div class="blog__list__post"><time class="blog__list__post__date">Apr 27 2023</time><br/><a href="/posts/2023-04-27-migrating-homeassistant-from-sd-to-ssd">Migrating Homeassistant from SD to SSD</a></div><div class="blog__list__post"><time class="blog__list__post__date">Apr 12 2023</time><br/><a href="/posts/2023-04-12-instant-dark-theme">Instant dark theme</a></div><div class="blog__list__post"><time class="blog__list__post__date">Mar 30 2023</time><br/><a href="/posts/2023-03-30-designing-resilient-cloud-infrastructure">Designing resilient cloud infrastructure</a></div></div></div></article></div><footer class="footer"><div class="footer__content"><h3>Links of Interest</h3><a href="/rss.xml">RSS Feed</a><br/><a href="/todo">Todo List</a><br/><a href="https://keyoxide.org/hkp/garrit@slashdev.space">PGP Key</a><br/><a href="/guestbook">Guestbook</a><br/><a href="/blogroll">Blogroll</a><br/><a href="/ctf">Capture the Flag</a><h3>Elsewhere</h3><a href="https://github.com/garritfra" rel="me">Github</a><br/><a href="https://www.linkedin.com/in/garritfranke/">LinkedIn</a><br/><a href="https://fosstodon.org/@garritfra">Mastodon (ActivityPub)</a><br/><a href="/contact">Contact</a></div><p>ðŸ‘»Â Proud member of <a target="_blank" href="https://darktheme.club/">darktheme.club</a> <!-- -->ðŸ‘»</p><p>Â© 2018-<!-- -->2023<!-- --> Garrit Franke<br/><a href="/privacy">Privacy</a> |<!-- --> <a target="_blank" href="https://github.com/garritfra/garrit.xyz">Source Code</a></p></footer></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"2021-02-07-storage-setup","markdownBody":"\nI used to rely on Google Drive and Photos to store my entire data. Now, that [Google has decided to ditch unlimited photo storage in the near future](https://blog.google/products/photos/storage-changes/) and Google basically being the devil himself, I decided to step up my game and get my hands dirty on a DIY storage solution.\n\n## The goal\n\nBefore I got started, I thought about the expectations I have towards a system like this. It boils down to these four points (in this order): I want my solution to be **resiliant**, **scalable**, **easy to maintain** and **easy to access**. Looking back, I think I met all of these requirements fairly well. Let me walk you through how I managed to do that.\n\n## Data resiliance\n\nKeeping data on a single device is obviously a really bad idea. Drives eventually fail, which means that your data will be lost. Heck, even my house could burn down, which means that any number of local copies could burn to ashes. To prevent data loss, I strictly adhere to the [3-2-1 backup strategy](https://www.backblaze.com/blog/the-3-2-1-backup-strategy/). A 3-2-1 strategy means having **at least three total copies of your data, two of which are local but on different mediums (read: devices), and at least one copy off-site**. If a drive fails, I can replace it. If my house burns down, I get two new drives and clone my offsite backup to them.\n\nTo get an offsite backup, I set up a spare Raspberry Pi with a single large HDD and instructed it to do daily backups of my entire data. I asked a family member if they would be willing to have a tiny computer plugged in to their router 24/7, and they kindly agreed. A Pi and a HDD are very efficient in terms of power, so there is not a lot to worry about.\n\n## Scalability\n\nI currently don't have a huge amount of data. If that were to change (i.e. if I continue to shoot a lot of high-res photos and shove them into my setup), I need a way to simply attach more drives, or ones with more capacity. I looked at different file-systems that allowed to easy extendability while also being resiliant.\n\nAn obvious candidate was **ZFS**, but there are a couple of reasons I ditched this idea. First of all, it is really hard to get up and running on Raspberry Pi running Linux, since it's not natively supported by all distributions. This increases the complexity of the setup. Another reason is that I don't like the way it scales. Please correct me if I'm wrong here, since I only did limited research on this. From what I know though, ZFS can only be extended by shoving a large amount of drives in the setup to achieve perfect redundancy.\n\nIn the end, I settled on **BTRFS**. For me, it scratches all the itches that ZFS has. It is baked into the linux kernel, which makes it really easy to install on most distributions, and I can scale it to any number of drives I want. If I find a spare drive somewhere with any storage capacity, I can plug it into the system and it will just work, without having to think about balancing or redundancy shenanigans.\n\n## Maintainability\n\nI need my setup to be easy to maintain. If a drive fails, I want to be able to replace it within a matter of minutes, not hours. If my host (a Raspberry Pi) bursts into flames, I want to be able to swap in a new one and still access my data. If I'm out and about and something goes south, I want to be able to fix it remotely. BTRFS helps a lot here. It's really the foundation for all the key points mentioned here. It gives me a simple interface to maintain the data on the drives, and tries to fix issues itself whenever possible.\n\nExposing random ports to the general public is a huge security risk. To still be able to access the Pi remotely, I set up **an encrypted WireGuard tunnel**. This way, I only have to expose a single port for WireGuard to talk to the device as if I'm sitting next to it.\n\n## Accessibility\n\nSince the data needs to be accessed frequently, I need a simple interface for it that can be used on any device. I decided to host a **Nextcloud** instance and mount the drive as external storage. Why external storage? Because Nextcloud does some weird thing with the data it stores. If I decide to ditch Nextcloud at some point, I have the data on the disks \"as is\", without some sort of abstraction on top of it. This also has the benefit of allowing access from multiple sources. I don't have to use Nextcloud, but instead can mount the volume as a FTP, SMB or NFS share and do whatever I want with it. From the nextcloud perspective, this has some drawbacks like inefficient caching or file detection, but I'm willing to make that tradeoff.\n\n## In a nutshell\n\nThis entire setup cost me about 150â‚¬ in total. Some components were scraped from old PC parts. So, what does the solution look like? Here is the gist:\n\n- A Raspberry Pi 4 as a main host and an older Raspberry Pi 3 for offsite backup, both running Raspberry Pi OS\n- Two external harddrives in a RAID 1 (mirrored) configuration, running on an external USB3 hub\n- A single internal HDD that served no purpose in my old PC, now serving as backup storage\n- All drives are using BTRFS\n- WireGuard tunnels between main and remote host, as well as most access devices\n- Nextcloud on main host, accessible over TLS (if I need to access data from outside the secure tunnel-network)\n- SMB share accessible from within the tunnel-network\n- Circa 4.5 terabyte total disk size; 1.5 terabyte of usable storage\n- Snapper for local incremental backups on main host; BTRBK for remote incremental backups\n- Cron jobs for regular backups and repairs (scrub/rebalance)\n\nThis is post 010 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"My storage setup (Feburary 2021)","date":"2021-02-07","tags":"infrastructure, guide, 100DaysToOffload, homelab"},"tags":["infrastructure","guide","100DaysToOffload","homelab"]},"recommendedPosts":[{"slug":"2023-06-01-single-page-applications-on-github-pages","markdownBody":"\nMy latest project, [sendpasswords.net](https://sendpasswords.net/) is a [Single Page Application](https://developer.mozilla.org/en-US/docs/Glossary/SPA) deployed on GitHub Pages.\n\nGitHub Pages is configured in a way to host static HTML files without any bells and whistles. This means that if you try to fetch a document that's *not* the index, for example `/foo`, the server will try to load the file with that name. \n\nBy nature, SPAs only consist of a single HTML entry point (`index.html` in most cases). It's responsible for routing the user to the correct page if there are multiple paths. And here's the crux: if the user tries to load `/foo`, he will not land at the SPA entry point. Instead, he will see a `404` error.\n\n## The solution\n\nA `404` response will automatically return a file called `404.html`, which we can use to our advantage. After building the application, simply copy the `index.html` to `404.html`, as demonstrated by [this commit](https://github.com/garritfra/sendpasswords.net/commit/66bdb68c229a3ac3386f7816a746155e658eb586). This will use `index.html` to serve the application on the root level, and `404.html` to load *the same app* if the page doesn't exist as a file. Whether the `index.html` is needed if there's already a `404.html` is up to you. I left it in to make clear that this is just a workaround.\n\nThis is a [well known](https://stackoverflow.com/a/69308662/9046809) workaround, but I wanted to bring some extra awareness to it, since it's a problem I ran into a couple of times so far. Happy SPAing!\n\n---\n\nThis is post 069 (nice) of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Single Page Applications on GitHub Pages","date":"2023-06-01","tags":"100DaysToOffload, guide, note, web, javascript, github"},"tags":["100DaysToOffload","guide","note","web","javascript","github"]},{"slug":"2023-04-28-serverless-framework-retrospective","markdownBody":"\nA current project requires the infrastructure to be highly scalable. It's expected that \u003e 50k Users hit the platform within a five minute period. Regular ECS containers take about one minute to scale up. That just won't cut it. I decided to go all in on the [serverless](https://www.serverless.com/) framework on AWS. Here's how it went.\r\n\r\n### Setup\r\n\r\nSetting up a serverless application was a breeze. You create a config file and use their CLI to deploy the app.\r\n\r\n### The rest of the infrastructure\r\n\r\nI decided to define the rest of the infrastructure (VPC, DB, cache, ...) in Terraform. But, since I wasn't familiar with how the Serverless Framework worked, I struggled to draw the line between what serverless should handle vs. what the rest of the infrastructure (Terraform) should provide. In a more traditional deployment workflow, you might let the CI deploy a container image to ECR and point the ECS service to that new image.\r\n\r\nI chose to let Serverless deploy the entire app through CI and build the rest of the infrastructure around it. The problem with this approach is that we lose fine-grained control over what's deployed where, which leads to a lot of permission errors.\r\n\r\nIn retrospect, I should've probably chosen the location of the S3 archive as the deployment target for the CI, and then point the lambda function to the location of the new artifact. This defeats the purpose of the framework, but it gives you a lot more control over your infrastructure. Once the next project comes along, I'll probably go that route instead.\r\n\r\n### Permissions\r\n\r\nServerless suggests to use admin permissions for deployments, and I see where they're coming from. Managing permissions in this framework is an absolute mess. Here's what the average deployment workflow looks like, if you want to use fine grained permissions:\r\n\r\n1. Wait for CloudFormation to roll back changes (~2 minutes)\r\n2. Update IAM role\r\n3. Deploy Serverless App\r\n4. If there's an error, go to 1\r\n\r\nThankfully, some people have already gone through the process of figuring this out. [Here's](https://serverlessfirst.com/create-iam-deployer-roles-serverless-app/#determining-deploy-time-permissions) a great guide with a starting point of the needed permissions.\r\n\r\n### Conclusion\r\n\r\nUsing the serverless framework is a solid choice if you just want to throw an app out there. Unfortunately the app I was deploying isn't \"just\" a dynamic website. The next time I'm building a serverless application it's probably not going to be with the Serverless Framework, though I learned a lot about serverless applications in general.\r\n\r\n---\r\n\r\nThis is post 067 of [#100DaysToOffload](https://100daystooffload.com/).\r\n\r\n\r\n\r\n\n","frontmatter":{"title":"Serverless Framework Retrospective","date":"2023-04-28","tags":"100DaysToOffload, infrastructure, aws, note, terraform, learnings, devops, serverless"},"tags":["100DaysToOffload","infrastructure","aws","note","terraform","learnings","devops","serverless"]},{"slug":"2023-04-27-migrating-homeassistant-from-sd-to-ssd","markdownBody":"\nI finally got frustrated with the performance of my Raspberry Pi 4 running Homeassistant on a SD card, so I went ahead and got an SSD.\r\n\r\nThe migration was **very** easy:\r\n\r\n1. Create and download a full backup through the UI\r\n2. Flash Homeassistant onto the SSD\r\n3. Remove the SD card and plug the SSD into a USB 3.0 port of the Pi\r\n4. Boot\r\n5. Go through the onboarding procedure\r\n6. Restore Backup\r\n7. Profit\r\n\r\nIt worked like a charm! The speed has improved A LOT, and everything was set up as it should be. \r\n\r\n...Until we turned on the lights in the livingroom. My ZigBee-dongle, plugged into another USB port, wasn't able to communicate with the devices on the network.\r\n\r\nAfter some digging around, I came across several threads stating that an SSD over USB 3.0 apparently creates a lot of interference to surrounding hardware, including my ZigBee dongle. The fix was simple: either get an extension port for the dongle, or plug the SSD into a USB 2.0 port of the Pi. Since I didn't have an extension cord to get the dongle far away enough from the SSD, I went with the latter option for now. And that fixed it! The performance was much worse, but still better than the SD I used before. My next step will be to grab an extension cord from my parents. I'm sure they won't mind.\r\n\r\nI hope this helps!\r\n\r\n---\r\n\r\nThis is post 066 of [#100DaysToOffload](https://100daystooffload.com/).\r\n\r\n\r\n\r\n\r\n\n","frontmatter":{"title":"Migrating Homeassistant from SD to SSD","date":"2023-04-27","tags":"100DaysToOffload, guide, note, homeassistant, homelab"},"tags":["100DaysToOffload","guide","note","homeassistant","homelab"]},{"slug":"2023-04-12-instant-dark-theme","markdownBody":"\nThanks to [Jacksons](https://jacksonchen666.com/) [update to darktheme.club](https://github.com/garritfra/darktheme.club/pull/79), I just came across a neat little [CSS property](https://developer.mozilla.org/en-US/docs/Web/CSS/color-scheme) that turns a mostly CSS-free document into a pleasantly dark site:\r\n\r\n```css\r\n:root {\r\n  color-scheme: light dark;\r\n}\r\n```\r\n\r\nThis will adjust all elements on the page to the color scheme preferred by the user - without any other custom styles! ðŸ¤¯ It is also [widely supported](https://caniuse.com/mdn-css_properties_color-scheme) by browsers.\r\n\r\nI've always been quite dependent on CSS-frameworks for any project I'm starting. Going forward, I'd be interested to see how framework-less sites would feel using this property. If all else fails, there's always the awesome [simple.css](https://simplecss.org/) library, which you can slap on top of a raw document to make it pretty (and dark, if preferred) without using custom classes.\r\n\r\n---\r\n\r\nThis is post 064 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Instant dark theme","date":"2023-04-12","tags":"100DaysToOffload, guide, note, learnings, web, css, til"},"tags":["100DaysToOffload","guide","note","learnings","web","css","til"]},{"slug":"2023-03-30-designing-resilient-cloud-infrastructure","markdownBody":"\r\nAs mentioned in a [previous post](/posts/2023-03-16-terraform-project-learnings), I'm currently finishing up building my first cloud infrastructure on AWS for a client at work. During the development, I learned a lot about designing components to be resilient and scalable. Here are some key takeaways.\r\n\r\nOne of the most critical components of a resilient infrastructure is redundancy. On AWS, you place your components inside a \"region\". This could be `eu-central-1` (Frankfurt) or `us-east-1` (North Virgina), etc. To further reduce the risk of an outage, each region is divided into multiple [Availability Zones](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html) (AZs). The AZs of a region are usually located some distance apart from each other. In case of a flood, a fire or a bomb detonating near one AZ, the other AZs should in most cases still be intact. You should have at least two, preferably three replicas of each component across multiple availability zones in a region. By having replicas of your components in different availability zones, you reduce the risk of downtime caused by an outage in a single availability zone.\r\n\r\nAnother way to ensure scalability and resilience for your database is to use [Aurora Serverless v2](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html). This database service is specifically designed for scalable, on-demand, and cost-effective performance. The database scales itself up or down based on the workload, which allows you to automatically and dynamically adjust the database capacity to meet the demand of your application, ensuring that your application is responsive and performs well without the need for manual intervention. Adding Serverless instances to an existing RDS cluster is also a seemless proccess.\r\n\r\nIn addition to switching to Aurora Serverless v2, using read replicas for cache and database in a separate availability zone can act as a hot standby without extra configuration. Keep in mind that read replicas are only utilized by explicitly using the read-only endpoint of a cluster. But even if you're only using the \"main\" cluster endpoint (and therefore just the primary instance), a read replica can promote itself to the primary instance in case of a fail over, which drastically reduces downtime.\r\n\r\nWhen using Amazon Elastic Container Service (ECS), use Fargate as opposed to EC2 instances. Fargate is a serverless compute engine for containers that allows you to run containers without having to manage the underlying infrastructure. It smartly locates instances across availability zones, ensuring that your application is always available.\r\n\r\nIn conclusion, you should always ensure that there are more than one instance of a component in your infrastructure. There are also services on AWS that abstract away the physical infrastructure (Fargate, S3, Lambda) and use a multi-AZ pattern by default.\r\n\r\n---\r\n\r\nThis is post 061 of [#100DaysToOffload](https://100daystooffload.com/).\r\n\r\n","frontmatter":{"title":"Designing resilient cloud infrastructure","date":"2023-03-30","tags":"100DaysToOffload, infrastructure, aws, guide, note, learnings"},"tags":["100DaysToOffload","infrastructure","aws","guide","note","learnings"]}]},"__N_SSG":true},"page":"/posts/[post]","query":{"post":"2021-02-07-storage-setup"},"buildId":"VYPhm2fTttAtwRaZriPSY","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>