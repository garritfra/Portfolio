{"pageProps":{"post":{"slug":"2022-09-26-self-hosted-software-im-thankful-for","markdownBody":"\nSelf-hosting software is not just rainbows and sunshine. I used to self-host a lot of my tools, but after some time the burden of maintaining those tools made me switch to hosted alternatives.\n\nHowever, there are a few projects that I stuck with over the years, and which I think deserve a deep appreciation.\n\n## Miniflux\n\n[Miniflux](https://miniflux.app/) is a very minimal, self-hostable RSS reader. It's been rock-solid since they day I started using it. The data for the application entirely lives In a Postgres database, which makes migrating the application to new infrastructure setups an absolute breeze. I've been meaning to support the author for quite some time now, but the cost of maintaining an instance yourself is basically zero, so I've yet to find the time to switch to their paid hosted instance.\n\n## Plausible Analytics\n\n[Plausible](https://plausible.io/) is another tool that just keeps on running. I haven't had any issues with it whatsoever, and I can't remember the last time I had to do a manual intervention. Just like Miniflux, there's a paid instance, which supports the author, and just like Miniflux, the software is so good that I haven't had a reason to switch to it yet. Oh, the irony.\n\n## BirdsiteLIVE\n\nWhile my instance of [BirdsiteLIVE](https://birdsite.slashdev.space/) is currently in a bad shape, this is not at all the fault of the software. There are limitations to the amount of Twitter API requests you can make, and I did a poor job managing the users on that instance. It's currently very overloaded and just very few tweets make it through. I will have to set aside some time to fix this, but the software itself has been rock solid since the day I started using it.\n\n## Synapse (Matrix)\n\nI was hesitant to mention [Matrix](https://matrix.org/) on this list. I had my ups and downs with Synapse (their Python implementation of a Matrix server), but the fact that my instance is still running after multiple infrastructure transitions and even a migration from SQLite to Postgres says something about the quality of the software. I have a feeling that Synapse is fairly resource-hungry, but if you feed it with enough RAM and disk, it will keep running indefinitely.\n\n## Homeassistant\n\nYou can throw [Homeassistant](https://www.home-assistant.io/) on a Raspberry Pi and everything works out of the gate. I even migrated my instance from a RPi 3 to a RPi 4 via their backup and restore functionality. It's absolutely flawless.\n\n## Dead projects\n\nI think it's fair to also mention the software that I no longer self-host.\n\n### E-Mail\n\nJust don't roll your own email.\n\n### Mastodon\n\nToo power hungry for my taste. No easy way to host inside docker, which made it a pain to keep running. I'm very happy with [Fosstodon](https://fosstodon.org/), and don't see a reason to switch to a self-hosted instance any time soon.\n\n### FreshRSS\n\nI tried replacing Miniflux once, but failed. Nothing beats Miniflux.\n\n### Prometheus + Grafana\n\nMonitoring **_inside_** your infra works until the infra goes down, at which point you're essentially driving blindfolded. I switched to Grafana Cloud, which includes a very generous free tier.\n\nThis is post 038 of [#100DaysToOffload](https://100daystooffload.com/).\n","frontmatter":{"title":"Self-hosted software I'm thankful for","date":"2022-09-26","tags":"note, 100DaysToOffload, infrastructure, homelab"},"tags":["note","100DaysToOffload","infrastructure","homelab"]},"recommendedPosts":[{"slug":"2023-04-28-serverless-framework-retrospective","markdownBody":"\nA current project requires the infrastructure to be highly scalable. It's expected that > 50k Users hit the platform within a five minute period. Regular ECS containers take about one minute to scale up. That just won't cut it. I decided to go all in on the [serverless](https://www.serverless.com/) framework on AWS. Here's how it went.\r\n\r\n### Setup\r\n\r\nSetting up a serverless application was a breeze. You create a config file and use their CLI to deploy the app.\r\n\r\n### The rest of the infrastructure\r\n\r\nI decided to define the rest of the infrastructure (VPC, DB, cache, ...) in Terraform. But, since I wasn't familiar with how the Serverless Framework worked, I struggled to draw the line between what serverless should handle vs. what the rest of the infrastructure (Terraform) should provide. In a more traditional deployment workflow, you might let the CI deploy a container image to ECR and point the ECS service to that new image.\r\n\r\nI chose to let Serverless deploy the entire app through CI and build the rest of the infrastructure around it. The problem with this approach is that we lose fine-grained control over what's deployed where, which leads to a lot of permission errors.\r\n\r\nIn retrospect, I should've probably chosen the location of the S3 archive as the deployment target for the CI, and then point the lambda function to the location of the new artifact. This defeats the purpose of the framework, but it gives you a lot more control over your infrastructure. Once the next project comes along, I'll probably go that route instead.\r\n\r\n### Permissions\r\n\r\nServerless suggests to use admin permissions for deployments, and I see where they're coming from. Managing permissions in this framework is an absolute mess. Here's what the average deployment workflow looks like, if you want to use fine grained permissions:\r\n\r\n1. Wait for CloudFormation to roll back changes (~2 minutes)\r\n2. Update IAM role\r\n3. Deploy Serverless App\r\n4. If there's an error, go to 1\r\n\r\nThankfully, some people have already gone through the process of figuring this out. [Here's](https://serverlessfirst.com/create-iam-deployer-roles-serverless-app/#determining-deploy-time-permissions) a great guide with a starting point of the needed permissions.\r\n\r\n### Conclusion\r\n\r\nUsing the serverless framework is a solid choice if you just want to throw an app out there. Unfortunately the app I was deploying isn't \"just\" a dynamic website. The next time I'm building a serverless application it's probably not going to be with the Serverless Framework, though I learned a lot about serverless applications in general.\r\n\r\n---\r\n\r\nThis is post 067 of [#100DaysToOffload](https://100daystooffload.com/).\r\n\r\n\r\n\r\n\n","frontmatter":{"title":"Serverless Framework Retrospective","date":"2023-04-28","tags":"100DaysToOffload, infrastructure, aws, note, terraform, learnings, devops, serverless"},"tags":["100DaysToOffload","infrastructure","aws","note","terraform","learnings","devops","serverless"]},{"slug":"2023-04-27-migrating-homeassistant-from-sd-to-ssd","markdownBody":"\nI finally got frustrated with the performance of my Raspberry Pi 4 running Homeassistant on a SD card, so I went ahead and got an SSD.\r\n\r\nThe migration was **very** easy:\r\n\r\n1. Create and download a full backup through the UI\r\n2. Flash Homeassistant onto the SSD\r\n3. Remove the SD card and plug the SSD into a USB 3.0 port of the Pi\r\n4. Boot\r\n5. Go through the onboarding procedure\r\n6. Restore Backup\r\n7. Profit\r\n\r\nIt worked like a charm! The speed has improved A LOT, and everything was set up as it should be. \r\n\r\n...Until we turned on the lights in the livingroom. My ZigBee-dongle, plugged into another USB port, wasn't able to communicate with the devices on the network.\r\n\r\nAfter some digging around, I came across several threads stating that an SSD over USB 3.0 apparently creates a lot of interference to surrounding hardware, including my ZigBee dongle. The fix was simple: either get an extension port for the dongle, or plug the SSD into a USB 2.0 port of the Pi. Since I didn't have an extension cord to get the dongle far away enough from the SSD, I went with the latter option for now. And that fixed it! The performance was much worse, but still better than the SD I used before. My next step will be to grab an extension cord from my parents. I'm sure they won't mind.\r\n\r\nI hope this helps!\r\n\r\n---\r\n\r\nThis is post 066 of [#100DaysToOffload](https://100daystooffload.com/).\r\n\r\n\r\n\r\n\r\n\n","frontmatter":{"title":"Migrating Homeassistant from SD to SSD","date":"2023-04-27","tags":"100DaysToOffload, guide, note, homeassistant, homelab"},"tags":["100DaysToOffload","guide","note","homeassistant","homelab"]},{"slug":"2023-03-30-designing-resilient-cloud-infrastructure","markdownBody":"\r\nAs mentioned in a [previous post](/posts/2023-03-16-terraform-project-learnings), I'm currently finishing up building my first cloud infrastructure on AWS for a client at work. During the development, I learned a lot about designing components to be resilient and scalable. Here are some key takeaways.\r\n\r\nOne of the most critical components of a resilient infrastructure is redundancy. On AWS, you place your components inside a \"region\". This could be `eu-central-1` (Frankfurt) or `us-east-1` (North Virgina), etc. To further reduce the risk of an outage, each region is divided into multiple [Availability Zones](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html) (AZs). The AZs of a region are usually located some distance apart from each other. In case of a flood, a fire or a bomb detonating near one AZ, the other AZs should in most cases still be intact. You should have at least two, preferably three replicas of each component across multiple availability zones in a region. By having replicas of your components in different availability zones, you reduce the risk of downtime caused by an outage in a single availability zone.\r\n\r\nAnother way to ensure scalability and resilience for your database is to use [Aurora Serverless v2](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html). This database service is specifically designed for scalable, on-demand, and cost-effective performance. The database scales itself up or down based on the workload, which allows you to automatically and dynamically adjust the database capacity to meet the demand of your application, ensuring that your application is responsive and performs well without the need for manual intervention. Adding Serverless instances to an existing RDS cluster is also a seemless proccess.\r\n\r\nIn addition to switching to Aurora Serverless v2, using read replicas for cache and database in a separate availability zone can act as a hot standby without extra configuration. Keep in mind that read replicas are only utilized by explicitly using the read-only endpoint of a cluster. But even if you're only using the \"main\" cluster endpoint (and therefore just the primary instance), a read replica can promote itself to the primary instance in case of a fail over, which drastically reduces downtime.\r\n\r\nWhen using Amazon Elastic Container Service (ECS), use Fargate as opposed to EC2 instances. Fargate is a serverless compute engine for containers that allows you to run containers without having to manage the underlying infrastructure. It smartly locates instances across availability zones, ensuring that your application is always available.\r\n\r\nIn conclusion, you should always ensure that there are more than one instance of a component in your infrastructure. There are also services on AWS that abstract away the physical infrastructure (Fargate, S3, Lambda) and use a multi-AZ pattern by default.\r\n\r\n---\r\n\r\nThis is post 061 of [#100DaysToOffload](https://100daystooffload.com/).\r\n\r\n","frontmatter":{"title":"Designing resilient cloud infrastructure","date":"2023-03-30","tags":"100DaysToOffload, infrastructure, aws, guide, note, learnings"},"tags":["100DaysToOffload","infrastructure","aws","guide","note","learnings"]},{"slug":"2023-03-16-terraform-project-learnings","markdownBody":"\r\nI just finished my first ever infrastructure project for a client. My Terraform skills are good enough to be dangerous, but during the development of this project I learned a lot that I would do differently next time.\r\n\r\n## Project structure\r\n\r\nHaving worked with semi-professional Terraform code before, I applied what I knew to my new project. That was mainly that we have a shared base and an overlay directory for each environment. I went with a single Terraform module for the shared infrastructure, and variables for each environment. Naively, roughly every service had their own file.\r\n\r\n```\r\n.\r\n├── modules\r\n│   └── infrastructure\r\n│       ├── alb.tf\r\n│       ├── cache.tf\r\n│       ├── database.tf\r\n│       ├── dns.tf\r\n│       ├── ecr.tf\r\n│       ├── ecs.tf\r\n│       ├── iam.tf\r\n│       ├── logs.tf\r\n│       ├── main.tf\r\n│       ├── network.tf\r\n│       ├── secrets.tf\r\n│       ├── security.tf\r\n│       ├── ssl.tf\r\n│       ├── state.tf\r\n│       └── variables.tf\r\n├── production\r\n│   ├── main.tf\r\n│   └── secrets.tf\r\n└── staging\r\n    ├── main.tf\r\n    └── secrets.tf\r\n```\r\n\r\nThis works very well, but I already started running into issues extending this setup. For my next project, I would probably find individual components and turn them into smaller reusable submodules. If I were to rewrite the project above, I would probably structure it like this (not a complete project, but I think you get the idea):\r\n\r\n```\r\n.\r\n├── modules\r\n│   └── infrastructure\r\n│       ├── main.tf\r\n│       ├── modules\r\n│       │   ├── database\r\n│       │   │   ├── iam.tf\r\n│       │   │   ├── logs.tf\r\n│       │   │   ├── main.tf\r\n│       │   │   ├── outputs.tf\r\n│       │   │   ├── rds.tf\r\n│       │   │   └── variables.tf\r\n│       │   ├── loadbalancer\r\n│       │   │   ├── alb.tf\r\n│       │   │   ├── logs.tf\r\n│       │   │   ├── main.tf\r\n│       │   │   ├── outputs.tf\r\n│       │   │   └── variables.tf\r\n│       │   ├── network\r\n│       │   │   ├── dns.tf\r\n│       │   │   ├── logs.tf\r\n│       │   │   ├── main.tf\r\n│       │   │   ├── outputs.tf\r\n│       │   │   ├── ssl.tf\r\n│       │   │   ├── variables.tf\r\n│       │   │   └── vpc.tf\r\n│       │   ├── service\r\n│       │   │   ├── ecr.tf\r\n│       │   │   ├── ecs.tf\r\n│       │   │   ├── iam.tf\r\n│       │   │   ├── logs.tf\r\n│       │   │   ├── main.tf\r\n│       │   │   ├── outputs.tf\r\n│       │   │   └── variables.tf\r\n│       │   └── state\r\n│       │       ├── locks.tf\r\n│       │       ├── main.tf\r\n│       │       ├── outputs.tf\r\n│       │       ├── s3.tf\r\n│       │       └── variables.tf\r\n│       ├── main.tf\r\n│       ├── outputs.tf\r\n│       └── variables.tf\r\n├── production\r\n│   ├── main.tf\r\n│   └── secrets.tf\r\n└── staging\r\n    ├── main.tf\r\n    └── secrets.tf\r\n```\r\n\r\n## Secrets\r\n\r\nI decided to use [git-crypt](https://github.com/AGWA/git-crypt) to manage secrets, but that was only before I learned about [SOPS](https://github.com/mozilla/sops). It's too late to migrate now, but if I could, I would choose SOPS for secrets any day of the week for upcoming projects. It even has a [Terraform provider](https://registry.terraform.io/providers/carlpett/sops/latest/docs), so there's no excuse not to use it. ;)\r\n\r\n## Conclusion\r\n\r\nOverall I'm pretty happy with how the project turned out, but there are some things that I learned during this project that will pay off later.\r\n\r\n---\r\n\r\nThis is post 057 of [#100DaysToOffload](https://100daystooffload.com/).\r\n","frontmatter":{"title":"Terraform project learnings","date":"2023-03-16","tags":"100DaysToOffload, infrastructure, aws, note, terraform, learnings"},"tags":["100DaysToOffload","infrastructure","aws","note","terraform","learnings"]},{"slug":"2023-03-10-debugging-ecs-tasks","markdownBody":"\r\nI just had to debug an application on AWS ECS. The whole procedure is documented in more detail in the [documentation](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-exec.html), but I think it's beneficial (both for my future self and hopefully to someone out there) to write down the proccess in my own words.\r\n\r\nFirst of all, you need access to the cluster via the [CLI](https://aws.amazon.com/de/cli/). In addition to the CLI, you need the [AWS Session Manager plugin for the CLI](https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html). If you're on MacOS, you can install that via [Homebrew](https://formulae.brew.sh/cask/session-manager-plugin):\r\n\r\n```\r\nbrew install --cask session-manager-plugin\r\n```\r\n\r\nNext, you need to allow the task you want to debug to be able to execute commands. Since I'm using Terraform, this was just a matter of adding the [`enable_execute_command`](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/ecs_service#enable_execute_command) attribute to the service:\r\n\r\n```tf\r\nresource \"aws_ecs_service\" \"my_service\" {\r\n  name            = \"my-service\"\r\n  cluster         = aws_ecs_cluster.my_cluster.id\r\n  task_definition = aws_ecs_task_definition.my_task_definition.id\r\n  desired_count   = var.app_count\r\n  launch_type     = \"FARGATE\"\r\n  enable_execute_command = true # TODO: Disable after debugging\r\n}\r\n```\r\n\r\nYou may also need specify an execution role in the task definition:\r\n\r\n```tf\r\nresource \"aws_ecs_task_definition\" \"my_task_definition\" {\r\n  family              = \"my-task\"\r\n  task_role_arn       = aws_iam_role.ecs_task_execution_role.arn\r\n  execution_role_arn  = aws_iam_role.ecs_task_execution_role.arn  # <-- Add this\r\n}\r\n```\r\n\r\nMake sure that this role has the correct access rights. There's a nice [troubleshooting guide](https://aws.amazon.com/de/premiumsupport/knowledge-center/ecs-error-execute-command/) going over the required permissions.\r\n\r\nIf you had to do some modifications, make sure to roll out a new deployment with the fresh settings:\r\n\r\n```\r\naws ecs update-service --cluster my-cluster --service my-service --force-new-deployment\r\n```\r\n\r\nNow, you should be able to issue commands against any running container!\r\n\r\n```\r\naws ecs execute-command --cluster westfalen --task <task-id-or-arn> --container my-container --interactive --command=\"/bin/sh\"\r\n```\r\n\r\nI hope this helps!\r\n\r\n---\r\n\r\nThis is post 055 of [#100DaysToOffload](https://100daystooffload.com/).\r\n","frontmatter":{"title":"Debugging ECS Tasks","date":"2023-03-10","tags":"100DaysToOffload, infrastructure, aws, guide, note, terraform"},"tags":["100DaysToOffload","infrastructure","aws","guide","note","terraform"]}]},"__N_SSG":true}